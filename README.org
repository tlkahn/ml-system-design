* ML System Design Primer
** Scope and frame the question
*** Loss function and metrics evaluations
** Data engineering
*** Sampling
*** Training pipeline
** Feature selection/engineering
*** One-hot encoding
**** Common problems
- Not work well with tree-based modesl: decision trees, random forests, etc.
  One-hot encoding can lead to high-dimensional sparse feature representations, which can negatively impact the performance of tree-based models. These models rely on splitting features based on thresholds, and high dimensionality can lead to inefficient splits and slower training. Additionally, tree-based models can handle categorical variables directly without one-hot encoding.
  #+begin_src python :tangle src/one-hot.py :comments link
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.preprocessing import OneHotEncoder

  # Load the Iris dataset
  iris = load_iris()
  X = iris.data
  y = iris.target

  # Split the data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42
  )

  # One-hot encode the features
  encoder = OneHotEncoder()
  X_train_encoded = encoder.fit_transform(X_train)
  X_test_encoded = encoder.transform(X_test)

  # Train a decision tree classifier on one-hot encoded data
  tree_model = DecisionTreeClassifier()
  tree_model.fit(X_train_encoded, y_train)

  # Evaluate the model on the test set
  accuracy = tree_model.score(X_test_encoded, y_test)
  print("Accuracy:", accuracy)
#+end_src
- Expensive computation/memory costs
*** Mean encoding
Mean encoding, also known as target encoding or likelihood encoding, is a technique used in machine learning to convert categorical variables into numerical representations. It involves replacing each category in a variable with the mean (or some other statistical measure) of the target variable for that category. This encoding can help capture the relationship between the categorical variable and the target variable, improving the performance of certain machine learning models.
*** Feature hashing
Feature hashing, also known as the hashing trick, is a technique used in machine learning to convert categorical or text features into numerical representations. It involves applying a hash function to the input features, which assigns them to a fixed number of buckets or bins. Each bin represents a unique combination of features, and the value in each bin is typically the count or frequency of that combination in the dataset. Feature hashing is useful when dealing with high-dimensional or sparse feature spaces, as it reduces the dimensionality of the data and can improve computational efficiency.

Some real-world examples include:

1. Text Classification: In natural language processing tasks like sentiment analysis or spam detection, feature hashing can be used to convert text features (such as words or n-grams) into numerical representations. This allows machine learning models to learn patterns and make predictions based on the hashed features.

2. Recommender Systems: Feature hashing can be used to handle high-dimensional categorical features, such as user or item IDs, in recommender systems. By converting these categorical features into numerical representations, feature hashing enables efficient computation and storage of large-scale recommendation models.

3. Click-through Rate (CTR) Prediction: In online advertising, feature hashing can be employed to handle the high-cardinality categorical features present in user demographics, ad properties, or context. By hashing these features, it reduces the dimensionality and allows for faster model training and prediction in CTR prediction models.

4. Fraud Detection: Feature hashing can be used to convert categorical features related to transactions, user behavior, or device information into numerical representations for fraud detection models. This helps capture patterns and relationships between features, enabling the model to detect fraudulent activities.

#+begin_src python :tangle src/feature-hashing.py :comments link
from sklearn.feature_extraction import FeatureHasher

# Example input data
data = [{'color': 'red', 'shape': 'circle'},
        {'color': 'blue', 'shape': 'triangle'},
        {'color': 'green', 'shape': 'square'}]

# Create a FeatureHasher object
hasher = FeatureHasher(n_features=10, input_type='dict')

# Transform the data
hashed_data = hasher.transform(data)

# Print the transformed features
print(hashed_data.toarray())
#+end_src
*** Cross feature
In machine learning, a cross feature, also known as an interaction
feature or interaction term, is a new feature that represents the
interaction or combination of multiple existing features. It captures
the relationship between different features and can provide additional
information for the model.

Here's an example of how to create cross features using the scikit-learn
library:

#+begin_src python :tangle src/cross-feature.py :comments link
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.linear_model import LinearRegression

  # Example input data
  X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
  y = [3, 5, 8]

  # Create PolynomialFeatures object with degree 2
  poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)

  # Generate cross features
  X_cross = poly.fit_transform(X)

  # Train a linear regression model
  reg = LinearRegression()
  reg.fit(X_cross, y)  # y represents the target variable
  reg.coef_
#+end_src

In the above example, we use the =PolynomialFeatures= class from
scikit-learn. The =degree= parameter specifies the maximum degree of
interaction terms to be generated. By setting =interaction_only=True=,
we only generate interaction terms without including the individual
features raised to powers. The =include_bias=False= parameter excludes
the bias term from the generated cross features.

The =fit_transform()= method of the =PolynomialFeatures= object
generates the cross features for the input data =X= and returns the
transformed data =X_cross=. We can then use these cross features to
train a machine learning model, such as =LinearRegression= in this case.

Note that cross features can help capture non-linear relationships or
interactions between features, but they can also increase the
dimensionality of the data, potentially leading to overfitting if not
used carefully.
*** Embedding
**** word2vec
***** CBOW
Continous Bag of Words (CBOW) is a model used in natural language processing (NLP) to generate word embeddings. It aims to predict a target word based on its context words within a given window size.

Here's an illustration of the CBOW model:

        Context Words
          (Input)
|-----------------------|
| word1   word2   word3 |
| CBOW Model            |
| (Word Embedding)      |
|                       |
| Target Word           |
| (Output)              |
|-----------------------|

In CBOW, the context words (word1, word2, word3 in the illustration) are provided as input to the model. The goal is to predict the target word given these context words.

The CBOW model consists of an embedding layer that maps each word to a fixed-size dense vector representation, often referred to as word embeddings. These word embeddings capture the semantic meaning of the words within a given context.

The word embeddings for the context words are averaged or summed up, and then passed through one or more hidden layers. These hidden layers learn to capture the relationships between the context words and predict the target word.

The output layer of the CBOW model predicts the target word using softmax or another activation function. The predicted target word is compared to the actual target word, and the model is trained to minimize the prediction error.

CBOW is commonly used in word2vec, a popular word embedding technique. It is efficient and works well when the target word can be accurately predicted based on the surrounding context words.
#+begin_src python :tangle src/cbow.py :comments link
  import numpy as np

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = set([word for sentence in data for word in sentence])
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  X_train = []
  y_train = []

  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []

          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(sentence[j])

          X_train.append(context_words)
          y_train.append(target_word)

  # Convert training data to one-hot vectors
  X_train_onehot = np.zeros((len(X_train), vocab_size), dtype=np.float32)
  y_train_onehot = np.zeros((len(y_train), vocab_size), dtype=np.float32)

  for i, context_words in enumerate(X_train):
      for word in context_words:
          X_train_onehot[i, word_to_index[word]] = 1

      y_train_onehot[i, word_to_index[y_train[i]]] = 1

  # Initialize weights
  input_dim = vocab_size
  hidden_dim = 10
  output_dim = vocab_size

  W1 = np.random.randn(input_dim, hidden_dim)
  W2 = np.random.randn(hidden_dim, output_dim)

  # Training loop
  learning_rate = 0.1
  epochs = 1000

  for epoch in range(epochs):
      # Forward pass
      hidden_layer = np.dot(X_train_onehot, W1)
      output_layer = np.dot(hidden_layer, W2)
      softmax_output = np.exp(output_layer) / np.sum(
          np.exp(output_layer), axis=1, keepdims=True
      )

      # Backward pass
      dW2 = np.dot(hidden_layer.T, (softmax_output - y_train_onehot))
      dW1 = np.dot(X_train_onehot.T, np.dot((softmax_output - y_train_onehot), W2.T))

      # Update weights
      W2 -= learning_rate * dW2
      W1 -= learning_rate * dW1

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(test_sentence[j])
      context.append(context_words)

  X_test = np.zeros((len(context), vocab_size), dtype=np.float32)
  for i, context_words in enumerate(context):
      for word in context_words:
          X_test[i, word_to_index[word]] = 1

  hidden_layer = np.dot(X_test, W1)
  output_layer = np.dot(hidden_layer, W2)
  softmax_output = np.exp(output_layer) / np.sum(
      np.exp(output_layer), axis=1, keepdims=True
  )

  predicted_word_index = np.argmax(softmax_output, axis=1)
  predicted_word = [list(vocab)[idx] for idx in predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src

Using Pytorch, we can do:

#+begin_src python :tangle src/cbow-ii.py :comments link
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, DataLoader

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = list(set([word for sentence in data for word in sentence]))
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  training_data = []
  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []
          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(word_to_index[sentence[j]])
                  training_data.append((context_words, word_to_index[target_word]))


  class CBOWDataset(Dataset):
      def __init__(self, data):
          self.data = data

      def __len__(self):
          return len(self.data)

      def __getitem__(self, index):
          context, target = self.data[index]
          return torch.tensor(context), torch.tensor(target)


  # CBOW model
  class CBOW(nn.Module):
      def __init__(self, vocab_size, embedding_dim, hidden_dim):
          super(CBOW, self).__init__()
          self.embedding = nn.Embedding(vocab_size, embedding_dim)
          self.fc1 = nn.Linear(embedding_dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, vocab_size)

      def forward(self, x):
          embedded = self.embedding(x).sum(dim=1)
          hidden = torch.relu(self.fc1(embedded))
          output = self.fc2(hidden)
          return output


  # Training parameters
  embedding_dim = 10
  hidden_dim = 10
  epochs = 100
  batch_size = 64
  learning_rate = 0.1

  # Create CBOW model instance
  model = CBOW(vocab_size, embedding_dim, hidden_dim)

  # Define loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(model.parameters(), lr=learning_rate)

  # Create DataLoader for training data
  train_dataset = CBOWDataset(training_data)
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

  # Training loop
  for epoch in range(epochs):
      running_loss = 0.0

      for context, target in train_loader:
          optimizer.zero_grad()

          output = model(context)
          loss = criterion(output, target)
          loss.backward()
          optimizer.step()

          running_loss += loss.item()

      print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(word_to_index[test_sentence[j]])
              context.append(context_words)

  model.eval()

  with torch.no_grad():
      context_tensor = torch.tensor(context)
      output = model(context_tensor)
      predicted_word_index = torch.argmax(output, dim=1).item()
      predicted_word = vocab[predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src
***** Skip-gram
Skip-gram is a model used in natural language processing (NLP) to generate word embeddings. Unlike the Continuous Bag of Words (CBOW) model, skip-gram aims to predict the context words given a target word.

During training, the model is optimized to maximize the probability of correctly predicting the context words. This is typically done using techniques like negative sampling or hierarchical softmax.
#+begin_src python :tangle src/skip-gram.py :comments link
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Sample input data
data = [['hello', 'world'],
        ['goodbye', 'world'],
        ['hello', 'goodbye'],
        ['world', 'hello']]

# Vocabulary
vocab = list(set([word for sentence in data for word in sentence]))
vocab_size = len(vocab)

# Word-to-index mapping
word_to_index = {word: i for i, word in enumerate(vocab)}

# Generate training data
training_data = []
for sentence in data:
    for i, target_word in enumerate(sentence):
        context_words = []
        for j in range(i - window_size, i + window_size + 1):
            if j != i and 0 <= j < len(sentence):
                context_words.append(word_to_index[sentence[j]])
        training_data.append((word_to_index[target_word], context_words))


class SkipGramDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        target, context = self.data[index]
        return torch.tensor(target), torch.tensor(context)


# Skip-gram model
class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGram, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Linear(embedding_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output = self.fc(embedded)
        return output


# Training parameters
embedding_dim = 10
epochs = 100
batch_size = 64
learning_rate = 0.1

# Create Skip-gram model instance
model = SkipGram(vocab_size, embedding_dim)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

# Create DataLoader for training data
train_dataset = SkipGramDataset(training_data)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(epochs):
    running_loss = 0.0

    for target, context in train_loader:
        optimizer.zero_grad()

        output = model(target)
        loss = criterion(output.view(-1, vocab_size), context.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

# Test the model
test_word = 'hello'
test_index = word_to_index[test_word]

model.eval()

with torch.no_grad():
    output = model(torch.tensor([test_index]))
    predicted_word_index = torch.argmax(output).item()
    predicted_word = vocab[predicted_word_index]

print("Predicted word:", predicted_word)
#+end_src
**** cotrained
Co-trained embedding, also known as joint embedding or multi-modal embedding, refers to the process of learning a shared representation space for multiple modalities or domains. It involves training an embedding model that can encode and align the information from different modalities, such as text, images, audio, or any other type of data.

The goal of co-trained embedding is to capture the similarities and relationships between different modalities in a common vector space. By doing so, it enables the model to perform various tasks that involve multiple modalities, such as cross-modal retrieval, image captioning, or text-to-image synthesis.

The process of co-trained embedding typically involves training a neural network architecture that can handle different types of input data. The network is designed to learn shared latent representations for each modality and optimize them jointly using a specific objective or loss function. This allows the model to align the embeddings of different modalities in a way that similar instances are closer together in the shared space.

Co-trained embedding models have been widely used in various applications, including multimedia information retrieval, cross-modal recommendation systems, and multimodal sentiment analysis. They provide a powerful approach to leverage the complementary information from different modalities, leading to improved performance and richer understanding of the data.
*** Positional embedding
*** Data leakage
Data leakage is when information from the target variable or future data unintentionally influences the training process, causing the model to have biased or overly optimistic predictions. It often occurs due to improper data preprocessing, splitting, or feature selection.
**** Causes
1. Temporal Leakage: Using future data in training, like predicting stock prices with data from after the prediction date.
   #+begin_src python :tangle src/data-leakage.py :comments link
     # Temporal leakage
     import pandas as pd

     # Load data
     data = pd.read_csv("stock_prices.csv")
     data["Date"] = pd.to_datetime(data["Date"])

     # Incorrect: Shuffling before splitting
     shuffled_data = data.sample(frac=1)
     train_data = shuffled_data[:800]
     test_data = shuffled_data[800:]

     # Correct: Sorting and splitting by date
     sorted_data = data.sort_values(by="Date")
     train_data = sorted_data[:800]
     test_data = sorted_data[800:]
   #+end_src
2. Target Leakage: Including target-related features in training, like predicting cancer from a dataset containing treatment information.
   #+begin_src python :tangle src/data-leakage.py :comments link
     # Target leakage
     from sklearn.model_selection import train_test_split

     # Load data
     data = pd.read_csv("cancer_data.csv")

     # Incorrect: Including target-related feature
     X = data[["Age", "Gender", "Tumor_Size", "Treatment"]]
     y = data["Cancer"]
     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

     # Correct: Excluding target-related feature
     X = data[["Age", "Gender", "Tumor_Size"]]
     y = data["Cancer"]
     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
   #+end_src
3. Improper Preprocessing: Applying transformations or scaling on the whole dataset before splitting, which transfers information between train and test sets. /Always split your data first before scaling, then use the statistics from the train split to scale all the splits... Leakage (also) might occur if the mean or median is calculated using entire data instead of just the train split./ [fn:1].
    #+begin_src python :tangle src/data-leakage.py :comments link
      # Improper preprocessing leakage
      from sklearn.preprocessing import StandardScaler

      # Load data
      data = pd.read_csv("data.csv")
      X = data.drop("target", axis=1)
      y = data["target"]

      # Incorrect: Scaling before splitting, leaking global and test statistics to train data
      scaler = StandardScaler()
      X_scaled = scaler.fit_transform(X)
      X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

      # Correct: Scaling after splitting
      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
      scaler = StandardScaler()
      X_train = scaler.fit_transform(X_train)
      X_test = scaler.transform(X_test)
    #+end_src
4. Group Leakage: Some data are closely related but end up in different train/validation/test groups.
5. Data generation leakage. Notice the data source and understanding how they are collected and processed.
**** Feature selection/pruning
***** Feature importance
| Interpretability Technique  | Type               |
|-----------------------------+--------------------|
| [[https://interpret.ml/docs/ebm.html][Explainable Boosting]]        | glassbox model     |
| [[https://interpret.ml/docs/dt.html][Decision Tree]]               | glassbox model     |
| [[https://interpret.ml/docs/dr.html][Decision Rule List]]          | glassbox model     |
| [[https://interpret.ml/docs/lr.html][Linear/Logistic Regression]]  | glassbox model     |
| [[https://interpret.ml/docs/shap.html][SHAP Kernel Explainer]]       | blackbox explainer |
| [[https://interpret.ml/docs/lime.html][LIME]]                        | blackbox explainer |
| [[https://interpret.ml/docs/msa.html][Morris Sensitivity Analysis]] | blackbox explainer |
| [[https://interpret.ml/docs/pdp.html][Partial Dependence]]          | blackbox explainer |
***** Feature generalization
Since the goal of an ML model is to make correct predictions on unseen data, features used for the model should generalize to unseen data. Not all features generalize equally. Overall, there are two aspects you might want to consider with regards to generalization: /feature coverage/ and /distribution of feature values./
- Coverage: A rough rule of thumb is that if this feature appears in a very small percentage of your data, it’s not going to be very generalizable.
- Distribution: If the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model’s performance.
** Model development and Offline evaluation
*** Ensembles
**** bagging
Given a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a smaller classifier or regression model on each of these bootstraps. If the problem is classification, the final prediction is decided by the majority vote of all models. If the problem is regression, the final prediction is the average of all models’ predictions.
[[./img/bagging.png]]
**** boosting
Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learn‐ ers focus more on the examples that previous weak learners misclassified.
[[./img/boosting.png]]
**** stacking
Stacking is an ensemble machine learning technique that combines multiple models' predictions using another model, called the meta-model. The base models are trained on the original data, while the meta-model is trained on the base models' predictions, which helps capture patterns and improve overall performance.

#+begin_src python :tangle src/stacking.py :comments link
  import numpy as np
  from sklearn.datasets import load_iris
  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score, recall_score, f1_score

  # Load data
  data = load_iris()
  X, y = data.data, data.target
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=28
  )

  # Base models
  model1 = RandomForestClassifier(random_state=36)
  model2 = GradientBoostingClassifier(random_state=98)

  # Train base models
  model1.fit(X_train, y_train)
  model2.fit(X_train, y_train)

  # Base models' predictions
  pred1 = model1.predict(X_train)
  pred2 = model2.predict(X_train)

  print(f"Accuracy of model 1: {accuracy_score(y_train, pred1)}")
  print(f"Accuracy of model 2: {accuracy_score(y_train, pred2)}")

  stacked_predictions_train = np.column_stack((pred1, pred2))

  # Train meta-model
  meta_model = LogisticRegression(random_state=12)
  meta_model.fit(stacked_predictions_train, y_train)

  # Test predictions
  test_pred1 = model1.predict(X_test)
  test_pred2 = model2.predict(X_test)
  stacked_predictions_test = np.column_stack((test_pred1, test_pred2))

  # Meta-model's final prediction
  final_prediction = meta_model.predict(stacked_predictions_test)

  # Accuracy
  print(f"Stacking Model Accuracy: {accuracy_score(y_test, final_prediction)}")
  print(f"Recall Score: {recall_score(y_test, final_prediction, average='macro')}")
  print(f"F1 Score: {f1_score(y_test, final_prediction, average='macro')}")
#+end_src
**** Experiment tracking and versioning
It’s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process. This enables you to compare different experiments and choose the one best suited for your needs. Comparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.
The process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. These two go hand in hand with each other. A large part of training an ML model is babysitting the learning processes. Many problems can arise during the training process, including loss not decreasing, overfitting, underfitting, fluctuating weight values, dead neurons, and running out of memory.
- loss curve
- model performance metrics: accuracy, F1, recall, perplexity[fn:2] etc.
- log of corresponding sample, prediction, and ground truth label.
- speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.
- System performance metrics such as memory usage and CPU/GPU utilization.
- The values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer) etc.
*** Distributed Training
*** DL model architecture
**** [[file:papers/Wide & Deep Learning for Recommender Systems.pdf][Wide and Deep Architecture]]
***** Architecture
[[file:./img/wide-and-deep-architecture.png]]
***** [[https://paperswithcode.com/paper/wide-deep-learning-for-recommender-systems#code][Paperwithcode]]
**** [[file:papers/youtube-multitask.pdf][Two-tower architecture]]
[[file:./img/two-tower.png]]
**** [[https://arxiv.org/pdf/2008.13535.pdf][Deep cross network]]
**** [[https://daiwk.github.io/assets/youtube-multitask.pdf][Multitask learning]]
**** [[https://arxiv.org/abs/1906.00091][Facebook DLRM]]
** Experiment
*** A/B testing
** Deployment

* Footnotes
[fn:2] Perplexity is a model performance metric used to evaluate the quality of language models, such as those used for natural language processing tasks. It measures how well the model predicts a given sample, with lower perplexity indicating a better fit. Essentially, it quantifies the average log-probability of the model's predictions, with a lower perplexity value implying a higher probability of predicting the correct words.
#+begin_src python :tangle src/perplexity.py :comments link
  import nltk
  from nltk.lm.preprocessing import padded_everygram_pipeline
  from nltk.lm import MLE
  from nltk.tokenize import word_tokenize, sent_tokenize
  from math import exp

  # Sample text
  text = "This is a sample text. We use it to demonstrate perplexity calculation."

  # Tokenize and preprocess
  sentences = sent_tokenize(text)
  tokenized_text = [word_tokenize(sent) for sent in sentences]
  n = 3  # Trigram model
  train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)

  # Train language model
  model = MLE(n)
  model.fit(train_data, padded_sents)

  # Test sentence
  test_sentence = "This is a test sentence."
  tokenized_test_sentence = word_tokenize(test_sentence)

  # Calculate perplexity
  test_ngrams = list(nltk.ngrams(tokenized_test_sentence, n))
  log_prob = 0

  for ngram in test_ngrams:
      if model.score(ngram[:-1], ngram[-1]) > 0:
          log_prob += -1 * model.logscore(ngram[-1], ngram[:-1])

  perplexity = exp(log_prob / len(test_ngrams))
  print(f"Perplexity: {perplexity}")
#+end_src

[fn:1] Chip Huyen, Designing Machine Learning Systems, p.138

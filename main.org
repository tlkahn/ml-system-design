** ML Primer
*** Feature selection/engineering
**** One-hot encoding
***** Common problems
- Not work well with tree-based modesl: decision trees, random forests, etc.
  One-hot encoding can lead to high-dimensional sparse feature representations, which can negatively impact the performance of tree-based models. These models rely on splitting features based on thresholds, and high dimensionality can lead to inefficient splits and slower training. Additionally, tree-based models can handle categorical variables directly without one-hot encoding.
  #+begin_src python :tangle src/one-hot.py
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.preprocessing import OneHotEncoder

  # Load the Iris dataset
  iris = load_iris()
  X = iris.data
  y = iris.target

  # Split the data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42
  )

  # One-hot encode the features
  encoder = OneHotEncoder()
  X_train_encoded = encoder.fit_transform(X_train)
  X_test_encoded = encoder.transform(X_test)

  # Train a decision tree classifier on one-hot encoded data
  tree_model = DecisionTreeClassifier()
  tree_model.fit(X_train_encoded, y_train)

  # Evaluate the model on the test set
  accuracy = tree_model.score(X_test_encoded, y_test)
  print("Accuracy:", accuracy)
#+end_src
- Expensive computation/memory costs
**** Mean encoding
Mean encoding, also known as target encoding or likelihood encoding, is a technique used in machine learning to convert categorical variables into numerical representations. It involves replacing each category in a variable with the mean (or some other statistical measure) of the target variable for that category. This encoding can help capture the relationship between the categorical variable and the target variable, improving the performance of certain machine learning models.
**** Feature hashing
Feature hashing, also known as the hashing trick, is a technique used in machine learning to convert categorical or text features into numerical representations. It involves applying a hash function to the input features, which assigns them to a fixed number of buckets or bins. Each bin represents a unique combination of features, and the value in each bin is typically the count or frequency of that combination in the dataset. Feature hashing is useful when dealing with high-dimensional or sparse feature spaces, as it reduces the dimensionality of the data and can improve computational efficiency.

Some real-world examples include:

1. Text Classification: In natural language processing tasks like sentiment analysis or spam detection, feature hashing can be used to convert text features (such as words or n-grams) into numerical representations. This allows machine learning models to learn patterns and make predictions based on the hashed features.

2. Recommender Systems: Feature hashing can be used to handle high-dimensional categorical features, such as user or item IDs, in recommender systems. By converting these categorical features into numerical representations, feature hashing enables efficient computation and storage of large-scale recommendation models.

3. Click-through Rate (CTR) Prediction: In online advertising, feature hashing can be employed to handle the high-cardinality categorical features present in user demographics, ad properties, or context. By hashing these features, it reduces the dimensionality and allows for faster model training and prediction in CTR prediction models.

4. Fraud Detection: Feature hashing can be used to convert categorical features related to transactions, user behavior, or device information into numerical representations for fraud detection models. This helps capture patterns and relationships between features, enabling the model to detect fraudulent activities.

#+begin_src python :tangle src/feature-hashing.py
from sklearn.feature_extraction import FeatureHasher

# Example input data
data = [{'color': 'red', 'shape': 'circle'},
        {'color': 'blue', 'shape': 'triangle'},
        {'color': 'green', 'shape': 'square'}]

# Create a FeatureHasher object
hasher = FeatureHasher(n_features=10, input_type='dict')

# Transform the data
hashed_data = hasher.transform(data)

# Print the transformed features
print(hashed_data.toarray())
#+end_src
**** Cross feature
In machine learning, a cross feature, also known as an interaction
feature or interaction term, is a new feature that represents the
interaction or combination of multiple existing features. It captures
the relationship between different features and can provide additional
information for the model.

Here's an example of how to create cross features using the scikit-learn
library:

#+begin_src python :tangle src/cross-feature.py
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.linear_model import LinearRegression

  # Example input data
  X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
  y = [3, 5, 8]

  # Create PolynomialFeatures object with degree 2
  poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)

  # Generate cross features
  X_cross = poly.fit_transform(X)

  # Train a linear regression model
  reg = LinearRegression()
  reg.fit(X_cross, y)  # y represents the target variable
  reg.coef_
#+end_src

In the above example, we use the =PolynomialFeatures= class from
scikit-learn. The =degree= parameter specifies the maximum degree of
interaction terms to be generated. By setting =interaction_only=True=,
we only generate interaction terms without including the individual
features raised to powers. The =include_bias=False= parameter excludes
the bias term from the generated cross features.

The =fit_transform()= method of the =PolynomialFeatures= object
generates the cross features for the input data =X= and returns the
transformed data =X_cross=. We can then use these cross features to
train a machine learning model, such as =LinearRegression= in this case.

Note that cross features can help capture non-linear relationships or
interactions between features, but they can also increase the
dimensionality of the data, potentially leading to overfitting if not
used carefully.
**** Embedding
***** word2vec
****** CBOW
Continous Bag of Words (CBOW) is a model used in natural language processing (NLP) to generate word embeddings. It aims to predict a target word based on its context words within a given window size.

Here's an illustration of the CBOW model:

        Context Words
          (Input)
|-----------------------|
| word1   word2   word3 |
| CBOW Model            |
| (Word Embedding)      |
|                       |
| Target Word           |
| (Output)              |
|-----------------------|

In CBOW, the context words (word1, word2, word3 in the illustration) are provided as input to the model. The goal is to predict the target word given these context words.

The CBOW model consists of an embedding layer that maps each word to a fixed-size dense vector representation, often referred to as word embeddings. These word embeddings capture the semantic meaning of the words within a given context.

The word embeddings for the context words are averaged or summed up, and then passed through one or more hidden layers. These hidden layers learn to capture the relationships between the context words and predict the target word.

The output layer of the CBOW model predicts the target word using softmax or another activation function. The predicted target word is compared to the actual target word, and the model is trained to minimize the prediction error.

CBOW is commonly used in word2vec, a popular word embedding technique. It is efficient and works well when the target word can be accurately predicted based on the surrounding context words.
#+begin_src python :tangle src/cbow.py
  import numpy as np

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = set([word for sentence in data for word in sentence])
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  X_train = []
  y_train = []

  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []

          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(sentence[j])

          X_train.append(context_words)
          y_train.append(target_word)

  # Convert training data to one-hot vectors
  X_train_onehot = np.zeros((len(X_train), vocab_size), dtype=np.float32)
  y_train_onehot = np.zeros((len(y_train), vocab_size), dtype=np.float32)

  for i, context_words in enumerate(X_train):
      for word in context_words:
          X_train_onehot[i, word_to_index[word]] = 1

      y_train_onehot[i, word_to_index[y_train[i]]] = 1

  # Initialize weights
  input_dim = vocab_size
  hidden_dim = 10
  output_dim = vocab_size

  W1 = np.random.randn(input_dim, hidden_dim)
  W2 = np.random.randn(hidden_dim, output_dim)

  # Training loop
  learning_rate = 0.1
  epochs = 1000

  for epoch in range(epochs):
      # Forward pass
      hidden_layer = np.dot(X_train_onehot, W1)
      output_layer = np.dot(hidden_layer, W2)
      softmax_output = np.exp(output_layer) / np.sum(
          np.exp(output_layer), axis=1, keepdims=True
      )

      # Backward pass
      dW2 = np.dot(hidden_layer.T, (softmax_output - y_train_onehot))
      dW1 = np.dot(X_train_onehot.T, np.dot((softmax_output - y_train_onehot), W2.T))

      # Update weights
      W2 -= learning_rate * dW2
      W1 -= learning_rate * dW1

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(test_sentence[j])
      context.append(context_words)

  X_test = np.zeros((len(context), vocab_size), dtype=np.float32)
  for i, context_words in enumerate(context):
      for word in context_words:
          X_test[i, word_to_index[word]] = 1

  hidden_layer = np.dot(X_test, W1)
  output_layer = np.dot(hidden_layer, W2)
  softmax_output = np.exp(output_layer) / np.sum(
      np.exp(output_layer), axis=1, keepdims=True
  )

  predicted_word_index = np.argmax(softmax_output, axis=1)
  predicted_word = [list(vocab)[idx] for idx in predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src

Using Pytorch, we can do:

#+begin_src python :tangle src/cbow-ii.py
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, DataLoader

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = list(set([word for sentence in data for word in sentence]))
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  training_data = []
  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []
          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(word_to_index[sentence[j]])
                  training_data.append((context_words, word_to_index[target_word]))


  class CBOWDataset(Dataset):
      def __init__(self, data):
          self.data = data

      def __len__(self):
          return len(self.data)

      def __getitem__(self, index):
          context, target = self.data[index]
          return torch.tensor(context), torch.tensor(target)


  # CBOW model
  class CBOW(nn.Module):
      def __init__(self, vocab_size, embedding_dim, hidden_dim):
          super(CBOW, self).__init__()
          self.embedding = nn.Embedding(vocab_size, embedding_dim)
          self.fc1 = nn.Linear(embedding_dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, vocab_size)

      def forward(self, x):
          embedded = self.embedding(x).sum(dim=1)
          hidden = torch.relu(self.fc1(embedded))
          output = self.fc2(hidden)
          return output


  # Training parameters
  embedding_dim = 10
  hidden_dim = 10
  epochs = 100
  batch_size = 64
  learning_rate = 0.1

  # Create CBOW model instance
  model = CBOW(vocab_size, embedding_dim, hidden_dim)

  # Define loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(model.parameters(), lr=learning_rate)

  # Create DataLoader for training data
  train_dataset = CBOWDataset(training_data)
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

  # Training loop
  for epoch in range(epochs):
      running_loss = 0.0

      for context, target in train_loader:
          optimizer.zero_grad()

          output = model(context)
          loss = criterion(output, target)
          loss.backward()
          optimizer.step()

          running_loss += loss.item()

      print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(word_to_index[test_sentence[j]])
              context.append(context_words)

  model.eval()

  with torch.no_grad():
      context_tensor = torch.tensor(context)
      output = model(context_tensor)
      predicted_word_index = torch.argmax(output, dim=1).item()
      predicted_word = vocab[predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src
****** Skip-gram
Skip-gram is a model used in natural language processing (NLP) to generate word embeddings. Unlike the Continuous Bag of Words (CBOW) model, skip-gram aims to predict the context words given a target word.

During training, the model is optimized to maximize the probability of correctly predicting the context words. This is typically done using techniques like negative sampling or hierarchical softmax.
#+begin_src python :tangle src/skip-gram.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Sample input data
data = [['hello', 'world'],
        ['goodbye', 'world'],
        ['hello', 'goodbye'],
        ['world', 'hello']]

# Vocabulary
vocab = list(set([word for sentence in data for word in sentence]))
vocab_size = len(vocab)

# Word-to-index mapping
word_to_index = {word: i for i, word in enumerate(vocab)}

# Generate training data
training_data = []
for sentence in data:
    for i, target_word in enumerate(sentence):
        context_words = []
        for j in range(i - window_size, i + window_size + 1):
            if j != i and 0 <= j < len(sentence):
                context_words.append(word_to_index[sentence[j]])
        training_data.append((word_to_index[target_word], context_words))


class SkipGramDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        target, context = self.data[index]
        return torch.tensor(target), torch.tensor(context)


# Skip-gram model
class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGram, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Linear(embedding_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output = self.fc(embedded)
        return output


# Training parameters
embedding_dim = 10
epochs = 100
batch_size = 64
learning_rate = 0.1

# Create Skip-gram model instance
model = SkipGram(vocab_size, embedding_dim)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

# Create DataLoader for training data
train_dataset = SkipGramDataset(training_data)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(epochs):
    running_loss = 0.0

    for target, context in train_loader:
        optimizer.zero_grad()

        output = model(target)
        loss = criterion(output.view(-1, vocab_size), context.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

# Test the model
test_word = 'hello'
test_index = word_to_index[test_word]

model.eval()

with torch.no_grad():
    output = model(torch.tensor([test_index]))
    predicted_word_index = torch.argmax(output).item()
    predicted_word = vocab[predicted_word_index]

print("Predicted word:", predicted_word)
#+end_src
***** cotrained
Co-trained embedding, also known as joint embedding or multi-modal embedding, refers to the process of learning a shared representation space for multiple modalities or domains. It involves training an embedding model that can encode and align the information from different modalities, such as text, images, audio, or any other type of data.

The goal of co-trained embedding is to capture the similarities and relationships between different modalities in a common vector space. By doing so, it enables the model to perform various tasks that involve multiple modalities, such as cross-modal retrieval, image captioning, or text-to-image synthesis.

The process of co-trained embedding typically involves training a neural network architecture that can handle different types of input data. The network is designed to learn shared latent representations for each modality and optimize them jointly using a specific objective or loss function. This allows the model to align the embeddings of different modalities in a way that similar instances are closer together in the shared space.

Co-trained embedding models have been widely used in various applications, including multimedia information retrieval, cross-modal recommendation systems, and multimodal sentiment analysis. They provide a powerful approach to leverage the complementary information from different modalities, leading to improved performance and richer understanding of the data.
*** Training pipeline
*** Loss function and metrics evaluations
*** Sampling
*** DL model architecture
*** A/B testing
*** Deployment patterns

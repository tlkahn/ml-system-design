#+HTML_HEAD: <link rel="stylesheet" href="./utils/styles/tufte.css" type="text/css" />
#+title: ML System Design Primer
#+bibliography: bib/main.bib
#+latex_header: \setlength{\parindent}{0pt}
#+latex_header: \usepackage[margin=2.5cm]{geometry}
#+latex_header: \usepackage{listings}
#+latex_header: \usepackage{xcolor}
#+LATEX_HEADER: \lstset{language=Python, basicstyle=\small\ttfamily, keywordstyle=\bfseries\color{blue}, commentstyle=\itshape\color[rgb]{0.133,0.545,0.133}, stringstyle=\color[rgb]{0.627,0.126,0.941}, showstringspaces=false, breaklines=true, frame=single, numbers=left, numberstyle=\tiny\color{gray}, stepnumber=1, numbersep=10pt, backgroundcolor=\color[rgb]{0.9,0.9,0.9}, tabsize=4, rulecolor=\color[rgb]{0.7,0.7,0.7}, captionpos=b, escapeinside={\%*}{*)}}
#+HTML_HEAD: <style type="text/css">#content{margin: 0;}</style>
* Scope and frame the question
** Loss function and metrics evaluations
References [fn:15] [fn:12]
* Data engineering
** Sampling
References [fn:13]
** Training pipeline
** Architecture
- Database
- SOA
- Event-based
  + pubsub
  + Message queue
* Feature selection/engineering
 Feature extraction refers to the process of transforming raw data into a more suitable format or representation for machine learning algorithms to work with. This can involve reducing the dimensionality, creating new features, or combining existing features to capture the underlying patterns in the data. Feature selection, on the other hand, focuses on selecting the most important features from the original dataset that contribute significantly to the model's performance. This is done to reduce overfitting, improve model interpretability, and decrease training time.

 Feature extraction is concerned with creating or transforming features, while feature selection is about identifying the most relevant features for a specific model.
** One-hot encoding
*** Common problems
- Not work well with tree-based modesl: decision trees, random forests, etc.
  One-hot encoding can lead to high-dimensional sparse feature representations, which can negatively impact the performance of tree-based models. These models rely on splitting features based on thresholds, and high dimensionality can lead to inefficient splits and slower training. Additionally, tree-based models can handle categorical variables directly without one-hot encoding.
  #+begin_src python :tangle src/one-hot.py :comments link
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.preprocessing import OneHotEncoder

  # Load the Iris dataset
  iris = load_iris()
  X = iris.data
  y = iris.target

  # Split the data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42
  )

  # One-hot encode the features
  encoder = OneHotEncoder()
  X_train_encoded = encoder.fit_transform(X_train)
  X_test_encoded = encoder.transform(X_test)

  # Train a decision tree classifier on one-hot encoded data
  tree_model = DecisionTreeClassifier()
  tree_model.fit(X_train_encoded, y_train)

  # Evaluate the model on the test set
  accuracy = tree_model.score(X_test_encoded, y_test)
  print("Accuracy:", accuracy)
#+end_src
- Expensive computation/memory costs
** Mean encoding
Mean encoding, also known as target encoding or likelihood encoding, is a technique used in machine learning to convert categorical variables into numerical representations. It involves replacing each category in a variable with the mean (or some other statistical measure) of the target variable for that category. This encoding can help capture the relationship between the categorical variable and the target variable, improving the performance of certain machine learning models.
** Feature hashing
Feature hashing [fn:14], also known as the hashing trick [cite:@freksen2018fully;@weinberger2010feature], is a technique used in machine learning to convert categorical or text features into numerical representations. It involves applying a hash function to the input features, which assigns them to a fixed number of buckets or bins. Each bin represents a unique combination of features, and the value in each bin is typically the count or frequency of that combination in the dataset. Feature hashing is useful when dealing with high-dimensional or sparse feature spaces, as it reduces the dimensionality of the data and can improve computational efficiency. It is also a core component for out-of-core support for libraries like Vowpal Wabbit [fn:7] and Tensorflow.

Some real-world examples include:

1. Text Classification: In natural language processing tasks like sentiment analysis or spam detection, feature hashing can be used to convert text features (such as words or n-grams) into numerical representations. This allows machine learning models to learn patterns and make predictions based on the hashed features.

2. Recommender Systems [cite:@wu2023survey;@Fan2023RecommenderSI;@Wang2023GenerativeRT;@Liu2023PretrainPA]: Feature hashing can be used to handle high-dimensional categorical features, such as user or item IDs, in recommender systems. By converting these categorical features into numerical representations, feature hashing enables efficient computation and storage of large-scale recommendation models.

3. Click-through Rate (CTR) Prediction: In online advertising, feature hashing can be employed to handle the high-cardinality categorical features present in user demographics, ad properties, or context. By hashing these features, it reduces the dimensionality and allows for faster model training and prediction in CTR prediction models. Here is an example:

   #+begin_src python :tangle feature-hashing-ctr-example.py :comments link
     from sklearn.feature_extraction import FeatureHasher
     from sklearn.linear_model import LogisticRegression
     from sklearn.model_selection import train_test_split
     from sklearn.metrics import accuracy_score

     # Example data
     data = [
         {"user_id": "user1", "ad_id": "ad1", "age": "25", "gender": "M", "clicked": 1},
         {"user_id": "user2", "ad_id": "ad2", "age": "30", "gender": "F", "clicked": 0},
         {"user_id": "user3", "ad_id": "ad3", "age": "35", "gender": "M", "clicked": 0},
         {"user_id": "user4", "ad_id": "ad4", "age": "40", "gender": "F", "clicked": 1},
     ]

     # Extract features and labels
     features = [{k: v for k, v in item.items() if k != "clicked"} for item in data]
     labels = [item["clicked"] for item in data]

     # Use FeatureHasher to handle high-cardinality categorical features
     hasher = FeatureHasher(n_features=20, input_type="dict")
     hashed_features = hasher.transform(features).toarray()

     # Split the data into training and testing sets
     X_train, X_test, y_train, y_test = train_test_split(
         hashed_features, labels, test_size=0.25, random_state=42
     )

     # Train a logistic regression model
     clf = LogisticRegression(solver="lbfgs")
     clf.fit(X_train, y_train)

     # Predict on the test set and calculate the accuracy
     y_pred = clf.predict(X_test)
     accuracy = accuracy_score(y_test, y_pred)

     print("Predicted values: ", y_pred)
     print("Accuracy: ", accuracy)
   #+end_src

4. Fraud Detection: Feature hashing can be used to convert categorical features related to transactions, user behavior, or device information into numerical representations for fraud detection models. This helps capture patterns and relationships between features, enabling the model to detect fraudulent activities.

    #+begin_src python :tangle src/feature-hashing.py :comments link
    from sklearn.feature_extraction import FeatureHasher
    import numpy as np

    # Example input data
    data = [
        {"color": "red", "shape": "circle"},
        {"color": "blue", "shape": "triangle"},
        {"color": "green", "shape": "square"},
    ]

    # Create a FeatureHasher object
    hasher = FeatureHasher(n_features=20, input_type="dict")

    # Transform the data
    hashed_data = hasher.transform(data)

    # Print the transformed features
    print(hashed_data.toarray())
    print(np.array(hashed_data.toarray()).shape)
    #+end_src


** Cross feature
In machine learning, a cross feature, also known as an interaction feature or interaction term, is a new feature that represents theinteraction or combination of multiple existing features. It capturesthe relationship between different features and can provide additionalinformation for the model.

Here's an example of how to create cross features using the scikit-learn library:

#+begin_src python :tangle src/cross-feature.py :comments link
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.linear_model import LinearRegression

  # Example input data
  X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
  y = [3, 5, 8]

  # Create PolynomialFeatures object with degree 2
  poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)

  # Generate cross features
  X_cross = poly.fit_transform(X)

  # Train a linear regression model
  reg = LinearRegression()
  reg.fit(X_cross, y)  # y represents the target variable
  reg.coef_
#+end_src

In the above example, we use the =PolynomialFeatures= class from scikit-learn. The =degree= parameter specifies the maximum degree of interaction terms to be generated. By setting =interaction_only=True=, we only generate interaction terms without including the individual features raised to powers. The =include_bias=False= parameter excludes the bias term from the generated cross features.

The =fit_transform()= method of the =PolynomialFeatures= object generates the cross features for the input data =X= and returns the transformed data =X_cross=. We can then use these cross features to train a machine learning model, such as =LinearRegression= in this case.

Note that cross features can help capture non-linear relationships or interactions between features, but they can also increase the dimensionality of the data, potentially leading to overfitting if not used carefully.

** Embedding
*** word2vec
**** CBOW
Continous Bag of Words (CBOW) is a model used in natural language processing (NLP) to generate word embeddings. It aims to predict a target word based on its context words within a given window size.

Here's an illustration of the CBOW model:

        Context Words
          (Input)
|-----------------------|
| word1   word2   word3 |
| CBOW Model            |
| (Word Embedding)      |
|                       |
| Target Word           |
| (Output)              |
|-----------------------|

In CBOW, the context words (word1, word2, word3 in the illustration) are provided as input to the model. The goal is to predict the target word given these context words.

The CBOW model consists of an embedding layer that maps each word to a fixed-size dense vector representation, often referred to as word embeddings. These word embeddings capture the semantic meaning of the words within a given context.

The word embeddings for the context words are averaged or summed up, and then passed through one or more hidden layers. These hidden layers learn to capture the relationships between the context words and predict the target word.

The output layer of the CBOW model predicts the target word using softmax or another activation function. The predicted target word is compared to the actual target word, and the model is trained to minimize the prediction error.

CBOW is commonly used in word2vec, a popular word embedding technique. It is efficient and works well when the target word can be accurately predicted based on the surrounding context words.
#+begin_src python :tangle src/cbow.py :comments link
  import numpy as np

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = set([word for sentence in data for word in sentence])
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  X_train = []
  y_train = []

  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []

          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(sentence[j])

          X_train.append(context_words)
          y_train.append(target_word)

  # Convert training data to one-hot vectors
  X_train_onehot = np.zeros((len(X_train), vocab_size), dtype=np.float32)
  y_train_onehot = np.zeros((len(y_train), vocab_size), dtype=np.float32)

  for i, context_words in enumerate(X_train):
      for word in context_words:
          X_train_onehot[i, word_to_index[word]] = 1

      y_train_onehot[i, word_to_index[y_train[i]]] = 1

  # Initialize weights
  input_dim = vocab_size
  hidden_dim = 10
  output_dim = vocab_size

  W1 = np.random.randn(input_dim, hidden_dim)
  W2 = np.random.randn(hidden_dim, output_dim)

  # Training loop
  learning_rate = 0.1
  epochs = 1000

  for epoch in range(epochs):
      # Forward pass
      hidden_layer = np.dot(X_train_onehot, W1)
      output_layer = np.dot(hidden_layer, W2)
      softmax_output = np.exp(output_layer) / np.sum(
          np.exp(output_layer), axis=1, keepdims=True
      )

      # Backward pass
      dW2 = np.dot(hidden_layer.T, (softmax_output - y_train_onehot))
      dW1 = np.dot(X_train_onehot.T, np.dot((softmax_output - y_train_onehot), W2.T))

      # Update weights
      W2 -= learning_rate * dW2
      W1 -= learning_rate * dW1

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(test_sentence[j])
      context.append(context_words)

  X_test = np.zeros((len(context), vocab_size), dtype=np.float32)
  for i, context_words in enumerate(context):
      for word in context_words:
          X_test[i, word_to_index[word]] = 1

  hidden_layer = np.dot(X_test, W1)
  output_layer = np.dot(hidden_layer, W2)
  softmax_output = np.exp(output_layer) / np.sum(
      np.exp(output_layer), axis=1, keepdims=True
  )

  predicted_word_index = np.argmax(softmax_output, axis=1)
  predicted_word = [list(vocab)[idx] for idx in predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src

Using Pytorch, we can do:

#+begin_src python :tangle src/cbow-ii.py :comments link
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, DataLoader

  # Sample input data
  data = [
      ["hello", "world"],
      ["goodbye", "world"],
      ["hello", "goodbye"],
      ["world", "hello"],
  ]

  # Vocabulary
  vocab = list(set([word for sentence in data for word in sentence]))
  vocab_size = len(vocab)

  # Word-to-index mapping
  word_to_index = {word: i for i, word in enumerate(vocab)}

  # Context window size
  window_size = 2

  # Generate training data
  training_data = []
  for sentence in data:
      for i, target_word in enumerate(sentence):
          context_words = []
          for j in range(i - window_size, i + window_size + 1):
              if j != i and 0 <= j < len(sentence):
                  context_words.append(word_to_index[sentence[j]])
                  training_data.append((context_words, word_to_index[target_word]))


  class CBOWDataset(Dataset):
      def __init__(self, data):
          self.data = data

      def __len__(self):
          return len(self.data)

      def __getitem__(self, index):
          context, target = self.data[index]
          return torch.tensor(context), torch.tensor(target)


  # CBOW model
  class CBOW(nn.Module):
      def __init__(self, vocab_size, embedding_dim, hidden_dim):
          super(CBOW, self).__init__()
          self.embedding = nn.Embedding(vocab_size, embedding_dim)
          self.fc1 = nn.Linear(embedding_dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, vocab_size)

      def forward(self, x):
          embedded = self.embedding(x).sum(dim=1)
          hidden = torch.relu(self.fc1(embedded))
          output = self.fc2(hidden)
          return output


  # Training parameters
  embedding_dim = 10
  hidden_dim = 10
  epochs = 100
  batch_size = 64
  learning_rate = 0.1

  # Create CBOW model instance
  model = CBOW(vocab_size, embedding_dim, hidden_dim)

  # Define loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(model.parameters(), lr=learning_rate)

  # Create DataLoader for training data
  train_dataset = CBOWDataset(training_data)
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

  # Training loop
  for epoch in range(epochs):
      running_loss = 0.0

      for context, target in train_loader:
          optimizer.zero_grad()

          output = model(context)
          loss = criterion(output, target)
          loss.backward()
          optimizer.step()

          running_loss += loss.item()

      print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

  # Test the model
  test_sentence = ["hello", "world"]
  context = []
  for i, target_word in enumerate(test_sentence):
      context_words = []
      for j in range(i - window_size, i + window_size + 1):
          if j != i and 0 <= j < len(test_sentence):
              context_words.append(word_to_index[test_sentence[j]])
              context.append(context_words)

  model.eval()

  with torch.no_grad():
      context_tensor = torch.tensor(context)
      output = model(context_tensor)
      predicted_word_index = torch.argmax(output, dim=1).item()
      predicted_word = vocab[predicted_word_index]

  print("Predicted word:", predicted_word)
#+end_src
**** Skip-gram
Skip-gram is a model used in natural language processing (NLP) to generate word embeddings. Unlike the Continuous Bag of Words (CBOW) model, skip-gram aims to predict the context words given a target word.

During training, the model is optimized to maximize the probability of correctly predicting the context words. This is typically done using techniques like negative sampling or hierarchical softmax.
#+begin_src python :tangle src/skip-gram.py :comments link
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Sample input data
data = [['hello', 'world'],
        ['goodbye', 'world'],
        ['hello', 'goodbye'],
        ['world', 'hello']]

# Vocabulary
vocab = list(set([word for sentence in data for word in sentence]))
vocab_size = len(vocab)

# Word-to-index mapping
word_to_index = {word: i for i, word in enumerate(vocab)}

# Generate training data
training_data = []
for sentence in data:
    for i, target_word in enumerate(sentence):
        context_words = []
        for j in range(i - window_size, i + window_size + 1):
            if j != i and 0 <= j < len(sentence):
                context_words.append(word_to_index[sentence[j]])
        training_data.append((word_to_index[target_word], context_words))


class SkipGramDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        target, context = self.data[index]
        return torch.tensor(target), torch.tensor(context)


# Skip-gram model
class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGram, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Linear(embedding_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output = self.fc(embedded)
        return output


# Training parameters
embedding_dim = 10
epochs = 100
batch_size = 64
learning_rate = 0.1

# Create Skip-gram model instance
model = SkipGram(vocab_size, embedding_dim)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

# Create DataLoader for training data
train_dataset = SkipGramDataset(training_data)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(epochs):
    running_loss = 0.0

    for target, context in train_loader:
        optimizer.zero_grad()

        output = model(target)
        loss = criterion(output.view(-1, vocab_size), context.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

# Test the model
test_word = 'hello'
test_index = word_to_index[test_word]

model.eval()

with torch.no_grad():
    output = model(torch.tensor([test_index]))
    predicted_word_index = torch.argmax(output).item()
    predicted_word = vocab[predicted_word_index]

print("Predicted word:", predicted_word)
#+end_src
*** Co-trained
Co-trained embedding, also known as joint embedding or multi-modal embedding, refers to the process of learning a shared representation space for multiple modalities or domains. It involves training an embedding model that can encode and align the information from different modalities, such as text, images, audio, or any other type of data.

The goal of co-trained embedding is to capture the similarities and relationships between different modalities in a common vector space. By doing so, it enables the model to perform various tasks that involve multiple modalities, such as cross-modal retrieval, image captioning, or text-to-image synthesis.

The process of co-trained embedding typically involves training a neural network architecture that can handle different types of input data. The network is designed to learn shared latent representations for each modality and optimize them jointly using a specific objective or loss function. This allows the model to align the embeddings of different modalities in a way that similar instances are closer together in the shared space.

Co-trained embedding models have been widely used in various applications, including multimedia information retrieval, cross-modal recommendation systems, and multimodal sentiment analysis. They provide a powerful approach to leverage the complementary information from different modalities, leading to improved performance and richer understanding of the data.

Here's a simple Python implementation of co-trained embedding for two modalities (text and images) using PyTorch. In this example, we'll use a pre-trained `ResNet18` model for image features and a `GloVe` embedding for text features. The goal is to learn a shared embedding space where similar text and images are close together.

#+begin_src python :tangle src/basic-cotrain.py :comments link
  import torch
  import torch.nn as nn
  import torchvision.models as models
  from torchtext.vocab import GloVe

  # Set up image and text feature extraction models
  resnet18 = models.resnet18(pretrained=True)
  resnet18 = nn.Sequential(*list(resnet18.children())[:-1])  # Remove classification layer
  glove = GloVe(name="6B", dim=300)


  # Co-trained embedding model
  class CoTrainedEmbedding(nn.Module):
      def __init__(self, text_dim, image_dim, embedding_dim):
          super(CoTrainedEmbedding, self).__init__()
          self.text_fc = nn.Linear(text_dim, embedding_dim)
          self.image_fc = nn.Linear(image_dim, embedding_dim)

      def forward(self, text, image):
          text_embed = self.text_fc(text)
          image_embed = self.image_fc(image)
          return text_embed, image_embed


  # Model parameters
  text_dim = 300  # GloVe 300-dimensional embedding
  image_dim = 512  # ResNet18 final feature map size
  embedding_dim = 128

  # Initialize the co-trained embedding model
  model = CoTrainedEmbedding(text_dim, image_dim, embedding_dim)

  # Example data
  text_data = "This is a sample text."
  image_data = torch.randn(1, 3, 224, 224)  # Random 224x224 image

  # Extract text and image features
  text_features = glove.get_vecs_by_tokens(text_data.split())
  image_features = resnet18(image_data).squeeze()

  # Forward pass through the co-trained embedding model
  text_embedding, image_embedding = model(text_features, image_features)
#+end_src
** Positional embedding
** Data leakage
Data leakage is when information from the target variable or future data unintentionally influences the training process, causing the model to have biased or overly optimistic predictions. It often occurs due to improper data preprocessing, splitting, or feature selection.
*** Causes
1. Temporal Leakage: Using future data in training, like predicting stock prices with data from after the prediction date.
   #+begin_src python :tangle src/data-leakage.py :comments link
     # Temporal leakage
     import pandas as pd

     # Load data
     df = pd.DataFrame(
         {
             "Date": ["2020-01-01", "2020-01-02", "2020-01-03", "2020-01-04"],
             "Stock_Price": [100, 101, 102, 103],
         }
     )
     data = pd.read_csv("stock_prices.csv")
     data["Date"] = pd.to_datetime(data["Date"])

     # Incorrect: Shuffling before splitting
     shuffled_data = data.sample(frac=1)
     train_data = shuffled_data[:800]
     test_data = shuffled_data[800:]

     # Correct: Sorting and splitting by date
     sorted_data = data.sort_values(by="Date")
     train_data = sorted_data[:800]
     test_data = sorted_data[800:]
   #+end_src
2. Target Leakage: Including target-related features in training, like predicting cancer from a dataset containing treatment information.
   #+begin_src python :tangle src/data-leakage.py :comments link
     # Target leakage
     from sklearn.model_selection import train_test_split

     data = pd.read_csv("cancer_data.csv")

     # Incorrect: Including target-related feature
     # X = data[["Age", "Gender", "Tumor_Size", "Treatment"]]
     # y = data["Cancer"]
     # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

     # Correct: Excluding target-related feature
     X = data[["Age", "Gender", "Tumor_Size"]]
     y = data["Cancer"]
     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
   #+end_src
3. Improper Preprocessing: Applying transformations or scaling on the whole dataset before splitting, which transfers information between train and test sets. /Always split your data first before scaling, then use the statistics from the train split to scale all the splits... Leakage (also) might occur if the mean or median is calculated using entire data instead of just the train split./ [fn:1].
    #+begin_src python :tangle src/data-leakage.py :comments link
      # Improper preprocessing leakage
      from sklearn.preprocessing import StandardScaler

      # Load data
      data = pd.read_csv("data.csv")
      X = data.drop("target", axis=1)
      y = data["target"]

      # Incorrect: Scaling before splitting, leaking global and test statistics to train data
      # scaler = StandardScaler()
      # X_scaled = scaler.fit_transform(X)
      # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
      scaler = StandardScaler()
      X_train = scaler.fit_transform(X_train)
      X_test = scaler.transform(X_test)
    #+end_src
4. Group Leakage: Some data are closely related but end up in different train/validation/test groups.
5. Data generation leakage. Notice the data source and understanding how they are collected and processed.
*** Feature selection/pruning
**** Feature importance
| Interpretability Technique  | Type               |
|-----------------------------+--------------------|
| [[https://interpret.ml/docs/ebm.html][Explainable Boosting]]        | glassbox model     |
| [[https://interpret.ml/docs/dt.html][Decision Tree]]               | glassbox model     |
| [[https://interpret.ml/docs/dr.html][Decision Rule List]]          | glassbox model     |
| [[https://interpret.ml/docs/lr.html][Linear/Logistic Regression]]  | glassbox model     |
| [[https://interpret.ml/docs/shap.html][SHAP Kernel Explainer]]       | blackbox explainer |
| [[https://interpret.ml/docs/lime.html][LIME]]                        | blackbox explainer |
| [[https://interpret.ml/docs/msa.html][Morris Sensitivity Analysis]] | blackbox explainer |
| [[https://interpret.ml/docs/pdp.html][Partial Dependence]]          | blackbox explainer |
**** Feature generalization
Since the goal of an ML model is to make correct predictions on unseen data, features used for the model should generalize to unseen data. Not all features generalize equally. Overall, there are two aspects you might want to consider with regards to generalization: /feature coverage/ and /distribution of feature values./
- Coverage: A rough rule of thumb is that if this feature appears in a very small percentage of your data, it’s not going to be very generalizable.
- Distribution: If the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model’s performance.
* Model development
** Classic ML
*** Exploratory data analysis (EDA)
Exploratory Data Analysis (EDA) is an essential step in the data analysis process, where the primary goal is to understand, summarize, and visualize the main characteristics and structure of a dataset. EDA involves examining the data, identifying patterns, detecting anomalies, and formulating hypotheses before building predictive models or conducting more formal statistical analyses.

EDA is an iterative and open-ended process that can involve various techniques, including:

1. **Descriptive statistics**: Compute summary statistics such as mean, median, mode, standard deviation, variance, skewness, and kurtosis to understand the central tendency, dispersion, and shape of the data distribution.

2. **Data cleaning**: Identify and handle missing values, outliers, and inconsistencies in the data. This process may involve imputation, data transformation, or removal of problematic data points.

3. **Data visualization**: Create visual representations of the data, such as histograms, box plots, scatter plots, bar charts, and heatmaps, to explore the relationships between variables, identify trends, and spot potential outliers or anomalies.

4. **Feature engineering**: Generate new features based on existing variables to improve the quality of the data and its ability to capture useful patterns. This can include creating interaction terms, aggregating variables, or applying dimensionality reduction techniques.

5. **Correlation analysis**: Investigate the relationships between variables by calculating correlation coefficients, such as Pearson's correlation coefficient for linear relationships or Spearman's rank correlation for monotonic relationships.

6. **Segmentation**: Divide the data into different groups or categories based on specific criteria, such as demographics or behavior patterns, to analyze the differences and similarities between the groups.

The main benefits of EDA are:

- Gaining a better understanding of the data's structure, relationships, and patterns, which can inform the choice of modeling techniques and feature engineering strategies.
- Identifying potential issues, such as missing values, outliers, and inconsistencies, that may affect the quality and reliability of the analysis.
- Generating hypotheses about the data that can be tested using more formal statistical methods or predictive models.

By conducting EDA, analysts and data scientists can make more informed decisions about the appropriate techniques to use in the subsequent steps of the data analysis process and increase the chances of obtaining meaningful and accurate results.

Below is an example of using tsne for EDA:

#+begin_src python :tangle src/simple-tsne.py :comments link
  import matplotlib.pyplot as plt
  import numpy as np
  import pandas as pd
  import seaborn as sns
  from sklearn import datasets
  from sklearn import manifold

  data = datasets.fetch_openml("mnist_784", version=1, return_X_y=True, parser="auto")
  pixel_values, targets = data
  targets = targets.astype(int)
  single_image = pixel_values.iloc[1, :].values.reshape(28, 28)
  plt.imshow(single_image, cmap="gray")
  plt.savefig("../img/single_mnist_image.png")
  plt.clf()

  tsne = manifold.TSNE(n_components=2, random_state=42)
  transformed_data = tsne.fit_transform(pixel_values.iloc[:100, :])

  tsne_df = pd.DataFrame(
      np.column_stack((transformed_data, targets[:100])), columns=["x", "y", "targets"]
  )
  # tsne_df.loc[:, "targets"] = tsne_df.targets.astype(int)
  grid = sns.FacetGrid(tsne_df, hue="targets")
  grid.map(sns.scatterplot, "x", "y").add_legend()
  plt.savefig("../img/tsne.png")
#+end_src
[[./img/single_mnist_image.png]]
[[./img/tsne.png]]
*** Cross validation
Here is an example of overfitting:
#+begin_src python :tangle src/overfitting-demo.py :comments link
  # import scikit-learn tree and metrics
  from sklearn import tree
  from sklearn import metrics

  # import matplotlib and seaborn # for plotting
  import matplotlib
  import matplotlib.pyplot as plt
  import seaborn as sns
  from scipy import stats
  from sklearn.datasets import load_wine
  import pandas as pd
  from sklearn.model_selection import train_test_split

  data = load_wine()
  # data.data # 178 * 13
  # stats.describe(data.data)
  # data.data.shape # 13 dimensions
  # data.target # {0, 1, 2}
  # this is our global size of label text # on the plots
  matplotlib.rc("xtick", labelsize=20)
  matplotlib.rc("ytick", labelsize=20)
  # This line ensures that the plot is displayed # inside the notebook
  # initialize lists to store accuracies # for training and test data
  # we start with 50% accuracy train_accuracies = [0.5] test_accuracies = [0.5]
  # iterate over a few depth values
  train_accuracies = test_accuracies = []
  train_data, test_data, train_labels, test_labels = train_test_split(
      data.data, data.target, test_size=0.3
  )
  for depth in range(1, 50):
      clf = tree.DecisionTreeClassifier(max_depth=depth)
      clf.fit(train_data, train_labels)
      train_predictions = clf.predict(train_data)
      test_predictions = clf.predict(test_data)
      train_accuracy = metrics.accuracy_score(train_labels, train_predictions)
      test_accuracy = metrics.accuracy_score(test_labels, test_predictions)
      train_accuracies.append(train_accuracy)
      test_accuracies.append(test_accuracy)

  # plot train_accuracies and test_accuracies
  plt.figure(figsize=(10, 5))
  sns.set_style("whitegrid")
  plt.plot(train_accuracies, label="train accuracy")
  plt.plot(test_accuracies, label="test accuracy")
  plt.savefig("../img/overfitting-demo.png")
  # plt.show()
#+end_src
[[./img/overfitting-demo.png]]
Cross-validation is a technique used to assess the performance of a machine learning model on unseen data. It helps overcome overfitting by providing a more accurate estimate of the model's performance on new data and ensuring that the model generalizes well to different subsets of the data.

Overfitting occurs when a model learns the training data too well, capturing noise and patterns that are not representative of the underlying data distribution. As a result, the model performs poorly on unseen data because it has essentially "memorized" the training data rather than learning to generalize from it.

Cross-validation can be categorized as follows:

- k-fold cross-validation
- stratified k-fold cross-validation
- hold-out based validation
- leave-one-out cross-validation
- group k-fold cross-validation

Here is an example of k-fold CV:
#+begin_src python :tangle src/basic-k-fold.py :comments link
  import pandas as pd
  from sklearn import model_selection
  from sklearn.utils import Bunch
  from sklearn import datasets

  # Training data is in a CSV file called train.csv
  iris_data = datasets.load_iris()
  # df.keys()
  # dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR',
  # df.data.shape  # (150, 4)
  # df.target.shape  # (150,)
  # df.target_names  # array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
  # df.feature_names  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
  # we create a new column called kfold and fill it with -1
  # create a pandas dataset from df.data
  df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
  df = df.sample(frac=1).reset_index(drop=True)
  # initiate the kfold class from model_selection module
  kf = model_selection.KFold(n_splits=5)
  # fill the new kfold column
  for fold, (trn_, val_) in enumerate(kf.split(X=df)):
      # trn_ and val_ are indexes of the training and validation subsets
      # like:
      #   [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47
      #   48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65
      #   66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83
      #   84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101
      #  102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
      #  120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
      #  138 139 140 141 142 143 144 145 146 147 148 149]
      #  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
      #  24 25 26 27 28 29]
      df.loc[val_, "kfold"] = fold
  # save the new csv with kfold column
  df.to_csv("../data/train_folds.csv", index=False)
#+end_src

The next important type of cross-validation is stratified k-fold. If you have a skewed dataset for binary classification with 90% positive samples and only 10% negative samples, you don't want to use random k-fold cross-validation. Using simple k-fold cross-validation for a dataset like this can result in folds with all negative samples. The rule is simple. If it’s a standard classification problem, choose stratified k-fold blindly.

In regression problems, the target variable is continuous, so you cannot directly use `StratifiedKFold`, which is designed for classification problems with categorical target variables. However, you can create a workaround by discretizing the continuous target variable into bins, as if it were a categorical variable, and then apply `StratifiedKFold`.

Here's an example using Python, pandas, and scikit-learn:

#+begin_src python :tangle src/basic-regression-k-fold.py :comments link
  import numpy as np
  import pandas as pd
  from sklearn.model_selection import StratifiedKFold

  # Create a sample dataset
  data = {
      "Feature1": np.random.rand(20),
      "Feature2": np.random.rand(20),
      "Target": np.random.rand(20),
  }
  df = pd.DataFrame(data)

  # Discretize the target variable into bins
  num_bins = 5
  labels = [f"Bin_{i}" for i in range(1, num_bins + 1)]
  df["Target_Bin"] = pd.cut(df["Target"], bins=num_bins, labels=labels)

  # Create StratifiedKFold
  stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

  # Split the dataset into folds
  for train_index, test_index in stratified_kfold.split(df, df["Target_Bin"]):
      train_set = df.iloc[train_index]
      test_set = df.iloc[test_index]
      print("Train set:\n", train_set, "\nTest set:\n", test_set, "\n---")
#+end_src

** Ensembles
*** bagging
Given a dataset [fn:16], instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a smaller classifier or regression model on each of these bootstraps. If the problem is classification, the final prediction is decided by the majority vote of all models. If the problem is regression, the final prediction is the average of all models’ predictions.
[[./img/bagging.png]]
*** boosting
Each learner in this ensemble is trained on the same set of samples, but the samples are weighted differently among iterations. As a result, future weak learn‐ ers focus more on the examples that previous weak learners misclassified.
[[./img/boosting.png]]
*** stacking
Stacking is an ensemble machine learning technique that combines multiple models' predictions using another model, called the meta-model. The base models are trained on the original data, while the meta-model is trained on the base models' predictions, which helps capture patterns and improve overall performance.

#+begin_src python :tangle src/stacking.py :comments link
  import numpy as np
  from sklearn.datasets import load_iris
  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score, recall_score, f1_score

  # Load data
  data = load_iris()
  X, y = data.data, data.target
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=28
  )

  # Base models
  model1 = RandomForestClassifier(random_state=36)
  model2 = GradientBoostingClassifier(random_state=98)

  # Train base models
  model1.fit(X_train, y_train)
  model2.fit(X_train, y_train)

  # Base models' predictions
  pred1 = model1.predict(X_train)
  pred2 = model2.predict(X_train)

  print(f"Accuracy of model 1: {accuracy_score(y_train, pred1)}")
  print(f"Accuracy of model 2: {accuracy_score(y_train, pred2)}")

  stacked_predictions_train = np.column_stack((pred1, pred2))

  # Train meta-model
  meta_model = LogisticRegression(random_state=12)
  meta_model.fit(stacked_predictions_train, y_train)

  # Test predictions
  test_pred1 = model1.predict(X_test)
  test_pred2 = model2.predict(X_test)
  stacked_predictions_test = np.column_stack((test_pred1, test_pred2))

  # Meta-model's final prediction
  final_prediction = meta_model.predict(stacked_predictions_test)

  # Accuracy
  print(f"Stacking Model Accuracy: {accuracy_score(y_test, final_prediction)}")
  print(f"Recall Score: {recall_score(y_test, final_prediction, average='macro')}")
  print(f"F1 Score: {f1_score(y_test, final_prediction, average='macro')}")
#+end_src
*** Experiment tracking and versioning
It’s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process. This enables you to compare different experiments and choose the one best suited for your needs. Comparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.
The process of tracking the progress and results of an experiment is called experiment tracking. The process of logging all the details of an experiment for the purpose of possibly recreating it later or comparing it with other experiments is called versioning. These two go hand in hand with each other. A large part of training an ML model is babysitting the learning processes. Many problems can arise during the training process, including loss not decreasing, overfitting, underfitting, fluctuating weight values, dead neurons, and running out of memory.
- loss curve
- model performance metrics: accuracy, F1, recall, perplexity etc.

  Perplexity is a model performance metric used to evaluate the quality of language models, such as those used for natural language processing tasks. It measures how well the model predicts a given sample, with lower perplexity indicating a better fit. Essentially, it quantifies the average log-probability of the model's predictions, with a lower perplexity value implying a higher probability of predicting the correct words.

#+begin_src python :tangle src/perplexity.py :comments link
    import torch
    import torch.nn as nn
    import torch.optim as optim


    # Create a simple model
    class SimpleModel(nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            self.fc = nn.Linear(10, 1)

        def forward(self, x):
            return self.fc(x)


    model = SimpleModel()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # Save a checkpoint
    torch.save(
        {
            "epoch": 5,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "loss": 0.05,
        },
        "checkpoint.pth",
    )

    # Load a checkpoint
    checkpoint = torch.load("checkpoint.pth")
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    epoch = checkpoint["epoch"]
    loss = checkpoint["loss"]

    model.train()  # Set the model in train mode
    # Continue training...
    #+end_src

- log of corresponding sample, prediction, and ground truth label.
- speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.
- System performance metrics such as memory usage and CPU/GPU utilization.
- The values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer) etc.
** Distributed Training
Some directions based on [cite:@nagrecha2023systems] include rematerialization[cite:@checkpointing2016], data spilling/CPU offloading [cite:@zero2019; @zero2021; @hydra2021; @mpms2021; @swapadvisor2021; @l2l2020], pipeline/model parallelism[cite:@gpipe2018; @pipedream2018; @terapipe2021; @torchgpipe2020; @megatronlmgpuscaling2021], and hybrid parallelism[cite:@flexflow2018; @alpa2022; @hydra2021; @mpms2021; @gshard2020]. These subjects, falling under the general umbrella of "large-model training techniques", has become a key focus for researchers across industry and academia, but the sheer volume of work in the space has made this topic difficult to navigate. This paper will provide a comprehensive review of the current state of the large-model DL training systems space, along with an assessment of future directions of growth and development in the area.
*** Rematerialization
*** Architecture parallelism
- data parallelism [fn:3]
- model parallelism
- pipeline parallelism
- hybrid parallelism
- Checkpointing

  Checkpointing in machine learning refers to the practice of periodically saving the state of a model during its training process. It usually involves storing the model's weights and other relevant information, such as optimizer state and training progress (i.e., the current epoch or iteration). Checkpointing serves several purposes:

  1. **Fault tolerance**: Training deep learning models can be a time-consuming and computationally expensive process. If the training process is interrupted due to hardware failure, software crash, or other issues, checkpointing allows you to resume training from the last saved state, avoiding the need to restart from scratch.

  2. **Early stopping**: Checkpointing enables you to monitor the model's performance on a validation set during training. If the validation performance starts degrading (indicating overfitting), you can stop the training early and revert to the best-performing checkpoint.

  3. **Model selection**: By saving checkpoints at different stages of the training process, you can evaluate and compare multiple versions of the model on a test set or based on various performance metrics. This allows you to select the best-performing model for deployment or further fine-tuning.

  4. **Resource management**: For long-running training processes or when using limited computational resources, checkpointing allows you to pause and resume training at your convenience. You can free up resources when needed and resume training when resources become available again.

  In deep learning frameworks like TensorFlow and PyTorch, you can implement checkpointing using built-in utilities for saving and loading model states. Here's a simple example of checkpointing using PyTorch:
    #+begin_src python :tangle src/basic-checkpointing.py :comments link
      import torch
      import torch.nn as nn
      import torch.optim as optim


      # Create a simple model
      class SimpleModel(nn.Module):
          def __init__(self):
              super(SimpleModel, self).__init__()
              self.fc = nn.Linear(10, 1)

          def forward(self, x):
              return self.fc(x)


      model = SimpleModel()
      optimizer = optim.SGD(model.parameters(), lr=0.01)

      # Save a checkpoint
      torch.save(
          {
              "epoch": 10,
              "model_state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": 0.1,
          },
          "checkpoint.pth",
      )

      # Load a checkpoint
      checkpoint = torch.load("checkpoint.pth")
      model.load_state_dict(checkpoint["model_state_dict"])
      optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
      epoch = checkpoint["epoch"]
      loss = checkpoint["loss"]

      model.train()  # Set the model in train mode
      # Continue training...
    #+end_src
*** Data Spilling/CPU offloading across the memory hierarchy
*** Memory efficient data representations
** Finetuning
  Fine-tuning [fn:6], especially in the context of modern deep learning models [cite:@lialin2023scaling; @dutt2023parameterefficient; @liao2023parameterefficient; @chung2022scaling; @lv2023parameter], refers to the process of adapting a pre-trained model to a new target task or dataset. Instead of training a model from scratch, which can be time-consuming and require large amounts of data, fine-tuning leverages the knowledge learned by the pre-trained model on a similar, larger dataset to achieve better performance on the target task with relatively less data and training time.

  Fine-tuning typically involves the following steps:

  1. Select a pre-trained model: Choose a model that has been trained on a large-scale dataset, usually in a similar domain or with similar characteristics to the target task. Examples of pre-trained models include BERT for natural language processing, ResNet for image classification, and Mask R-CNN for object detection and segmentation.

  2. Modify the model architecture: Adjust the model's architecture to fit the target task's requirements. This often involves replacing or modifying the last layers of the model, such as the classification or regression layers, to match the desired output dimensions or classes.

  3. Initialize with pre-trained weights: Load the pre-trained weights into the modified model, ensuring that the new layers are initialized with random weights or suitable initialization schemes.

  4. Fine-tune the model: Train the modified model on the target dataset, usually with a lower learning rate compared to training from scratch. This is to prevent the model from unlearning the useful features learned during pre-training. You can choose to update all the model's weights or only the weights of the newly added layers, depending on the similarity of the target task to the pre-training task and the available computational resources.

  Here's an example of fine-tuning a pre-trained ResNet-18 model for a new image classification task using PyTorch:

  #+begin_src python :tangle src/basic-finetuning.py :comments link
    import torch
    import torch.nn as nn
    import torchvision.models as models
    import torchvision.datasets as datasets
    import torchvision.transforms as transforms

    # Load a pre-trained ResNet-18 model
    resnet18 = models.resnet18(pretrained=True)

    # Modify the model architecture for the new task
    num_classes = 100  # Number of target classes
    resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)

    # Load the target dataset
    transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    train_dataset = datasets.ImageFolder("path/to/train_data", transform=transform)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=32, shuffle=True, num_workers=4
    )

    # Set up the loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)

    # Fine-tune the model
    num_epochs = 10
    resnet18.train()
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            outputs = resnet18(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
  #+end_src
- Repurposing
  + Knowledge injection [fn:5]
  + Adapter modules
** Model compression
...three main approaches to reduce its inference latency: make it do inference faster, make the model smaller, or make the hardware it’s deployed on run faster. The process of making a model smaller is called model compression, and the process to make it do inference faster is called inference optimization.

- Low-Rank Factorization: [cite:@hu2021lora] the key idea behind low-rank factorization is to replace high-dimensional tensors with lower-dimensional tensors. A Example: compact convolutional filters: where the over-parameterized (having too many parameters) convolution filters are replaced with compact blocks to both reduce the number of parameters and increase speed.

- Knowledge distillation: a method in which a small model (student) is trained to mimic a larger model or ensemble of models (teacher). The smaller model is what you’ll deploy.

- Pruning: a method originally used for decision trees where you remove sections of a tree that are uncritical and redundant for classification.25 As neural networks gained wider adoption, people started to realize that neural networks are over-parameterized and began to find ways to reduce the workload caused by the extra parameters. One is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters. The more common meaning is to find parameters least useful to predictions and set them to 0. In this case, pruning doesn’t reduce the total number of parameters, only the number of nonzero parameters. The architecture of the neural network remains the same. This helps with reducing the size of a model because pruning makes a neural network more sparse, and sparse architecture tends to require less storage space than dense structure.

- Quantization: the most general and commonly used model compression method. Quantization reduces a model’s size by using fewer bits to represent its parameters. [fn:4]

- Dynamic shape input: in deep learning, dynamic shape input refers to the ability of a neural network to handle input data with varying shapes or dimensions during the training and inference process. Traditional neural networks typically require fixed-size inputs, meaning the input data must be reshaped or preprocessed to fit the expected dimensions before being fed to the network.

  - Dynamic shape input is useful when dealing with data like images, text, or time-series data, where the dimensions can vary significantly. For example, when processing images, you might encounter images with different aspect ratios, sizes, or channels. Similarly, in text processing, the length of sentences or documents can also differ significantly.

  - To handle dynamic shape inputs, modern deep learning frameworks like TensorFlow and PyTorch provide mechanisms to support variable-sized input data. These mechanisms include:

  1. Dynamic computation graph: Unlike their static counterparts, dynamic computation graphs can be constructed on-the-fly during runtime, allowing for input data with varying shapes to be handled more easily.

  2. Padding and masking: Padding is a technique to resize inputs to a fixed shape by adding extra elements (like zeros), while masking is used to ignore the padded elements during the computation process. This is particularly useful for handling sequences with varying lengths, such as sentences or time-series data.

  3. Tensor reshaping and broadcasting: Deep learning frameworks provide functions to reshape and broadcast tensors, making it possible to manipulate input data with varying shapes more easily.

  - By incorporating dynamic shape input support into a deep learning model, you can create more flexible and adaptable models that can handle real-world data with varying dimensions.

  1. Handling variable-length sequences with padding and masking:

     #+begin_src python
       import torch
       import torch.nn as nn


       # Create a simple RNN model
       class SimpleRNN(nn.Module):
           def __init__(self, input_size, hidden_size, num_layers):
               super(SimpleRNN, self).__init__()
               self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)

           def forward(self, x, lengths):
               # Pack padded sequence
               x_packed = nn.utils.rnn.pack_padded_sequence(
                   x, lengths, batch_first=True, enforce_sorted=False
               )

               # Forward pass through RNN
               out_packed, _ = self.rnn(x_packed)

               # Unpack packed sequence
               out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)
               return out


       # Input data (batch_size=2, variable sequence_length, input_size=3)
       x1 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])

       x2 = torch.tensor([[10.0, 11.0, 12.0], [13.0, 14.0, 15.0]])

       # Pad sequences to the same length
       x_padded = nn.utils.rnn.pad_sequence([x1, x2], batch_first=True)
       lengths = [x1.size(0), x2.size(0)]

       # Create the RNN model
       input_size = 3
       hidden_size = 5
       num_layers = 1
       model = SimpleRNN(input_size, hidden_size, num_layers)

       # Forward pass
       output = model(x_padded, lengths)
       print(output)
     #+end_src

  2. Handling variable-sized images with adaptive pooling:

     #+begin_src python
       import torch
       import torch.nn as nn


       # Create a simple CNN model with adaptive pooling
       class SimpleCNN(nn.Module):
           def __init__(self, num_classes):
               super(SimpleCNN, self).__init__()
               self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
               self.pool = nn.MaxPool2d(2, 2)
               self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
               self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
               self.fc = nn.Linear(32, num_classes)

           def forward(self, x):
               x = self.pool(F.relu(self.conv1(x)))
               x = self.pool(F.relu(self.conv2(x)))
               x = self.adaptive_pool(x)
               x = torch.flatten(x, 1)
               x = self.fc(x)
               return x


       # Input data (variable-sized images with 3 channels)
       image1 = torch.randn(3, 64, 64)  # 64x64 image
       image2 = torch.randn(3, 128, 128)  # 128x128 image
       image3 = torch.randn(3, 96, 48)  # 96x48 image

       images = [image1.unsqueeze(0), image2.unsqueeze(0), image3.unsqueeze(0)]

       # Create the CNN model
       num_classes = 10
       model = SimpleCNN(num_classes)

       # Forward pass for each image
       for image in images:
           output = model(image)
           print(output)
     #+end_src

  - In these examples, we demonstrate how to handle dynamic shape input for sequences with variable lengths and images with different sizes using PyTorch. The first example employs padding and masking to handle variable-length sequences in an RNN model, while the second example uses adaptive pooling to handle variable-sized images in a CNN model.

- Caching [fn:9]

** Offline Evaluation
*** Metrics
F1, precision, and recall are asymmetric metrics because they don't treat false positives and false negatives equally. Precision focuses on the proportion of true positives among predicted positives, while recall focuses on the proportion of true positives among actual positives. F1 score is the harmonic mean of precision and recall, balancing the two metrics. As a result, they emphasize different aspects of a model's performance.

  #+begin_src python :tangle src/evaluation-metrics.py :comments link
    from sklearn.metrics import precision_score, recall_score, f1_score
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression

    # Generate synthetic binary classification data
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(
      - X, y, test_size=0.2, random_state=42
    )

    # Train a classifier
    clf = LogisticRegression(random_state=42)
    clf.fit(X_train, y_train)

    # Predictions
    y_pred = clf.predict(X_test)

    # Calculate precision, recall, and F1 score
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")
    print(f"Confusion Matrix:\n{cm}")

    # Modify predictions to increase false positives
    y_pred_modified = np.copy(y_pred)
    y_pred_modified[:20] = 1  # Force the first 20 instances to be positive

    # Recalculate metrics
    precision_modified = precision_score(y_test, y_pred_modified)
    recall_modified = recall_score(y_test, y_pred_modified)
    f1_modified = f1_score(y_test, y_pred_modified)

    print("\nModified Metrics:")
    print(f"Precision: {precision_modified}") # (ref:precision)
    print(f"Recall: {recall_modified}")
    print(f"F1 Score: {f1_modified}")
  #+end_src

  1. [[(precision)][Precision]]: (True Positives) / (True Positives + False Positives)
     - High precision means fewer false positives.
     - If precision is low, the model incorrectly classifies many negative instances as positive.

  2. Recall: (True Positives) / (True Positives + False Negatives)
     - High recall means fewer false negatives.
     - If recall is low, the model misses many positive instances, classifying them as negative.

  3. F1 Score: 2 * (Precision * Recall) / (Precision + Recall)
     - Balances precision and recall, considering both false positives and false negatives.

  4. Precision at k (P@K):

     The metric is particularly useful in the context of recommendation systems and search engines, to evaluate the relevance of the top k items or results returned by a model. It is the proportion of the top k items that are relevant or correct.

     Precision at k is defined as follows:

     P@k = (Number of relevant items in the top k items) / k

     For example, consider a recommendation system that recommends movies to users. Suppose the system returns the following recommendations for a user:

     - Recommended movies: [M1, M2, M3, M4, M5]
     - Relevant movies for the user: [M1, M4, M6]

     Here, we can calculate the Precision at k for different values of k:

        - P@1 = $\frac{1}{1}$ = 1.0 (since M1 is relevant)
        - P@2 = $\frac{1}{2}$ = 0.5 (since M1 is relevant, but M2 is not)
        - P@3 = $\frac{1}{3}$ = 0.33 (since M1 is relevant, but M2 and M3 are not)
        - P@4 = $\frac{2}{4}$ = 0.5 (since M1 and M4 are relevant)
        - P@5 = $\frac{2}{5}$ = 0.4 (since M1 and M4 are relevant, but M5 is not)

       In this example, P@k gives us insight into the quality of the top k recommendations, indicating how many of them are relevant to the user.

  5. Average precision at k (AP@K):

    AP@k takes into account both the precision at various cutoff points (k) and the ordering of the items in the list.

    AP@k is defined as the average of the precision values obtained at each position in the ranked list where a relevant item is found, up to position k:

    \[AP@k = \frac{1}{\text{Number of relevant items}} \sum_{i=1}^k (P(i) \cdot rel(i))\]

    - $P(i)$ is the precision at position i in the ranked list.
    - $rel(i)$ is an indicator function, which is equal to 1 if the item at position $i$ is relevant, and 0 otherwise.

    The AP@k metric gives higher scores to ranked lists where relevant items appear earlier in the list. It combines both precision and recall by considering the position of relevant items and their ordering in the ranked list.

    For example, consider a recommendation system that recommends movies to users. Suppose the system returns the following recommendations for a user:

    - Recommended movies: [M1, M2, M3, M4, M5]
    - Relevant movies for the user: [M1, M4, M6]

    In this case, we can calculate the AP@5 as follows:

    - P(1) = $\frac{1}{1}$ = 1.0 (since M1 is relevant)
    - P(2) = $\frac{1}{2}$ = 0.5 (since M1 is relevant, but M2 is not)
    - P(3) = $\frac{1}{3}$ = 0.33 (since M1 is relevant, but M2 and M3 are not)
    - P(4) = $\frac{2}{4}$ = 0.5 (since M1 and M4 are relevant)

    $AP@5 = (1 / 2) * (1.0 * 1 + 0.5 * 1) = 0.75$

    In this example, the AP@5 score is 0.75, indicating the quality of the recommendations and their ordering in the ranked list.

    The asymmetry of these metrics can be observed in situations where one metric is high while the other is low. For example, a model with high precision but low recall is good at avoiding false positives but misses many actual positive instances. Conversely, a model with high recall but low precision correctly identifies most positive instances but generates many false positives. Understanding these asymmetries helps you choose an appropriate metric based on your specific problem or application's requirements. For instance, in a medical diagnosis system, you might prioritize high recall to avoid missing positive cases, even if it means more false positives. Conversely, in a spam detection system, you might focus on high precision to reduce false positives, ensuring that legitimate emails don't end up in the spam folder.

    Here are some circumstances that require high precision and high recall:

    1. High Precision: High precision is important when the cost of false positives is high or when you want to be very confident about the positive predictions made by the model. Examples of situations that require high precision include:

        - Email spam filtering: You don't want to accidentally classify important emails as spam (false positives), as this could cause users to miss important messages.
        - Fraud detection: Incorrectly flagging a transaction as fraudulent (false positive) can lead to customer dissatisfaction and lost business. It is important to minimize false positives for a better user experience.

    2. High Recall: High recall is important when the cost of false negatives is high or when you want to capture as many positive instances as possible. Examples of situations that require high recall include:

        - Medical diagnosis: Missing a positive case of a serious disease (false negative) can have severe consequences for the patient. A high recall ensures that positive cases are identified even at the expense of some false positives.
        - Search engines: Users expect search engines to return all relevant documents for a query. High recall ensures that the search engine retrieves as many relevant results as possible, even if some irrelevant results are also returned.

    It is possible to have both high precision and high recall, but there is often a trade-off between the two. Increasing precision may lead to a decrease in recall, and vice versa. The trade-off arises from the fact that increasing the threshold for classifying positive instances in a model often results in fewer false positives (increasing precision) but more false negatives (decreasing recall), and decreasing the threshold has the opposite effect. The balance between precision and recall depends on the specific problem and the desired outcomes. In some cases, it may be more important to prioritize one over the other, while in other cases, a balance between the two is desired. In practice, the relationship between precision and recall is often visualized using a Precision-Recall curve, which shows how precision and recall change as the classification threshold is varied. This curve helps to identify the optimal threshold for the desired balance between precision and recall in a specific application.

    #+begin_src python :tangle src/precision-recall-curve.py :comments link
      import numpy as np
      from sklearn.datasets import make_classification
      from sklearn.linear_model import LogisticRegression
      from sklearn.model_selection import train_test_split
      import matplotlib.pyplot as plt

      # Create a sample dataset
      X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

      # Split the dataset into train and test sets
      X_train, X_test, y_train, y_test = train_test_split(
          X, y, test_size=0.25, random_state=42
      )

      # Train a logistic regression model
      model = LogisticRegression(solver="liblinear", random_state=42)
      model.fit(X_train, y_train)

      from sklearn.metrics import precision_recall_curve

      # Get predicted probabilities for the positive class
      y_scores = model.predict_proba(X_test)[:, 1]

      # Compute the Precision-Recall curve
      precision, recall, thresholds = precision_recall_curve(y_test, y_scores)
      plt.figure()
      plt.plot(recall, precision, marker=".")
      num_thresholds_to_display = 5
      threshold_indices = np.linspace(
          0, len(thresholds) - 1, num_thresholds_to_display, dtype=int
      )

      for i in threshold_indices:
          plt.annotate(
              f"Thresh: {thresholds[i]:.2f}",
              xy=(recall[i], precision[i]),
              xytext=(recall[i] - 0.1, precision[i] + 0.02),
              arrowprops={
                  "facecolor": "black",
                  "arrowstyle": "wedge,tail_width=0.7",
                  "lw": 1,
                  "alpha": 0.5,
              },
              fontsize=9,
              color="black",
          )

      # plt.xlabel("Recall")
      # plt.ylabel("Precision")
      # plt.title("recision-Recall Curve")

      from sklearn.metrics import roc_curve, auc

      # Compute the ROC curve
      fpr, tpr, thresholds = roc_curve(y_test, y_scores)
      roc_auc = auc(fpr, tpr)

      # Plot the ROC curve
      plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")
      plt.xlabel("False Positive Rate")
      plt.ylabel("True Positive Rate")
      plt.title("ROC Curve")
      plt.legend(loc="lower right")
      plt.savefig("../img/precision-recall-roc-example.png")
    #+end_src

    [[./img/precision-recall-roc-example.png]]

    The term "recall" is used for this metric because it measures the model's ability to "recall" or "remember" the true positive instances among all the actual positive instances (i.e., the fraction of true positives out of all actual positives). In other words, recall quantifies how well the model can identify the relevant cases in the dataset, hence the name.

*** Baselines
Evaluation metrics, by themselves, means little. When evaluating your model, it’s essential to know the baseline you’re evaluating it against.
- random baseline
- simple heuristic baseline
- zero rule baseline
  - A special case of the simple heuristic baseline when your baseline model always predicts the most common class
- human baseline
- existing solutions
*** Methods
**** perturbation tests
- The more sensitive your model is to noise, the harder it will be to maintain it.
**** invariance tests
- Excluding sensitive information or adding/removing/changing irrelavant dimensions should not affect the predictions.
**** directional expectation tests
- Models with changed data in dimensions should not generate predictions with directions against common sense.
**** model calibration
- If a model predicts that team A will beat team B with a 70% probability, and out of the 1,000 times these two teams play together, team A only wins 60% of the time, then we say that this model isn’t calibrated. A calibrated model should predict that team A wins with a 60% probability. To quote Nate Silver in his book The Signal and the Noise, calibration is "one of the most important tests of a forecast— I would argue that it is the single most important one."
- To measure a model’s calibration [fn:11], a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y.
- To calibrate your models, a common method is [[https://en.wikipedia.org/wiki/Platt_scaling][Platt scaling]], which is implemented in scikit-learn with sklearn.calibration.CalibratedClassifierCV.
**** confidence measurement
- If you only want to show the predictions that your model is certain about, how do you measure that certainty?
**** slice-based evaluation
- Slicing means to separate your data into subsets and look at your model’s performance on each subset separately.
- heuristics-based
- error analysis
- slice finder
** Model Architecture
*** [[file:papers/Wide & Deep Learning for Recommender Systems.pdf][Wide and Deep Architecture]]
**** Architecture
- [[file:./img/wide-and-deep-architecture.png]]
**** [[https://paperswithcode.com/paper/wide-deep-learning-for-recommender-systems#code][Paperwithcode]]
*** [[file:papers/youtube-multitask.pdf][Two-tower architecture]]
- [[file:./img/two-tower.png]]
*** [[https://arxiv.org/pdf/2008.13535.pdf][Deep cross network]]
*** [[https://daiwk.github.io/assets/youtube-multitask.pdf][Multitask learning]]
*** [[https://arxiv.org/abs/1906.00091][Facebook DLRM]]
*** [[https://www.pytorchlightning.ai/index.html][Pytorch lightning]]
** Experiment
*** A/B testing
*** Bayesian optimization
* Deployment
- ...deploying is easy if you ignore all the hard parts. If you want to deploy a model for your friends to play with, all you have to do is to wrap your predict function in a POST request endpoint using Flask or FastAPI, put the dependencies this predict function needs to run in a container,2 and push your model and its associated container to a cloud service like AWS or GCP to expose the endpoint:
    #+begin_src python
    # Example of how to use FastAPI to turn your predict function # into a POST endpoint
    @app.route("/predict", methods=["POST"])
    def predict():
        X = request.get_json()["X"]
        y = MODEL.predict(X).tolist()
        return json.dumps({"y": y}), 200
    #+end_src
- Hard parts:
  + Latency
  + Throughput
  + Uptime
  + Logginga and monitoring
  + MLOps
** Unification of batch, online and stream processing
* Data Distribution shifts and monitoring
** Degenerate feedback loops
- popularity bias (exposure bias, filter bubbles and sometimes echo chambers) [cite:@Jiang_2019; @Chia_2022; @abdollahpouri2019managing; @fleder2007blockbuster; @brynjolfsson2011goodbye]
- metrics: aggregate diversity, average coverage of long-tail items, bucket-based accuracy
- countering measures:
  + randomization
    This is the approach that TikTok follows. Each new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This pool of traffic is used to evaluate each video’s unbiased quality to determine whether it should be moved to a bigger pool of traffic or be marked as irrelevant. [fn:2]
    There are some variations to improve on the accuracy loss: Contextual bandits [cite:@guo2020deep; @Li_2010] as an exploration strategy [fn:8]. [cite:@schnabel2016recommendations] uses a small amount of randomization and causal inference techniques to estimate the unbiased value of each song.
  + positional/rank features (Google's Rule 36 [fn:17])
  + Intention harvesting [cite:@Agarwal_2019; @Aslanyan_2019]
    The key idea is that logged user engagement data in a matured ranking system already contains the ranks from multiple different ranking models, for example from historic A/B tests or simply from different versions of the production model that have been rolled out over time. This historic diversity creates an inherent randomness in ranks, which we can “harvest” to estimate position bias, without any costly interventions.
- Other biases:
    + clickbait bias [fn:10] means that the model is biased in favor of clickbait content
    + duration bias means that the model is biased in favor of long videos (and against short videos)
    + popularity bias means that the model is biased in favor of popular content instead of the unique interests of a particular user
    + single-interest bias means that the model fails to learn multiple user interests at the same time
    + survivorship bias means that the model is biased in favor of content that has survived the filtering process
** Data distribution shifts
Data distribution shift refers to the phenomenon in supervised learning when the data a model works with changes over time, which causes this model’s predictions to become less accurate as time passes. The distri‐ bution of the data the model is trained on is called the source distribution. The distribution of the data the model runs inference on is called the target distribution.
*** Types
- covariate shift
  When P(X) changes but P(Y|X) remains the same.
- label shift
  When P(Y) changes but P(X|Y) remains the same.
- concept drift
  When P(Y|X) changes but P(X) remains the same.
- feature change
- Label schema change
*** Detecting Data Distribution Shifts
- Monitoring metrics in production. Need access to ground truth labels which is typically delayed.
- When no access to labels, use proxies. The distributions of interest are the input distribution $P(X)$, the label distribution $P(Y)$, and the conditional distributions $P(X|Y)$ and $P(Y|X)$. In the industry, most drift detection methods focus on detecting changes in the input distribution, especially the distributions of features.
  - Statistical methods
    + Simple statistics: compare their statistics like min, max, mean, median, variance, various quantiles (such as 5th, 25th, 75th, or 95th quantile), skewness, kurtosis.
    + Two-sample hypothesis test: It’s a test to determine whether the difference between two popu‐ lations (two sets of data) is statistically significant. A caveat is that just because the difference is statistically significant doesn’t mean that it is practically important. A basic two-sample test is the Kolmogorov–Smirnov test, also known as the K-S or KS test. However it won’t work for high-dimensional data, and features are usually high-dimensional. Alibi Detect is a great open source package with the implementations of many drift detection algorithms, as shown below:
        | Detector                         | Tabular | Image | Time Series | Text | Categorical Features | Online | Feature Level |
        |----------------------------------+---------+-------+-------------+------+----------------------+--------+---------------|
        | Kolmogorov-Smirnov               | O       | O     |             | O    | O                    |        | O             |
        | Cramér-von Mises                 | O       | O     |             |      |                      | O      | O             |
        | Fisher's Exact Test              | O       |       |             |      | O                    | O      | O             |
        | Maximum Mean Discrepancy (MMD)   | O       | O     |             | O    | O                    | O      |               |
        | Learned Kernel MMD               | O       | O     |             | O    | O                    |        |               |
        | Context-aware MMD                | O       | O     | O           | O    | O                    |        |               |
        | Least-Squares Density Difference | O       | O     |             | O    | O                    | O      |               |
        | Chi-Squared                      | O       |       |             |      | O                    |        | O             |
        | Mixed-type tabular data          | O       |       |             |      | O                    |        | O             |
        | Classifier                       | O       | O     | O           | O    | O                    |        |               |
        | Spot-the-diff                    | O       | O     | O           | O    | O                    |        | O             |
        | Classifier Uncertainty           | O       | O     | O           | O    | O                    |        |               |
        | Regressor Uncertainty            | O       | O     | O           | O    | O                    |        |               |
  - Time scale windows for detecting shifts
    Temporal shifts are shifts that happen over time. To detect temporal shifts, a common approach is to treat input data to ML applications as time-series data. [cite:@ramanan2021realtime]
*** Addressing data drift
- Train models using massive datasets. The hope here is that if the training dataset is large enough, the model will be able to learn such a comprehensive distribution that whatever data points the model will encounter in production will likely come from this distribution.
- Adapt a trained model to a target distribution without requiring new labels. [cite:@pmlr-v97-zhao19a; @lipton2018detecting; @10.5555/3042817.3043028] Haven’t found wide adoption in industry.
- Retrain your model using the labeled data from the target distribution, which can mean retraining from scratch on both the old and new data or continuing training the existing model on new data. The latter approach is also called [[*Finetuning][fine-tuning]] (which is also applied in domain adaptation and transfer learning).
- It’s possible to design your system to make it more robust to shifts. A system uses multiple features, and different features shift at different rates. When choosing features for your models, you might want to consider the trade-off between the performance and the stability of a feature.

** Monitoring and observability (instrumentation)
* Test in production
Core ask: How often should I retrain my models? The rational for /test in production/: This process is a way to test your systems with live data in production to ensure that your updated model indeed works without catastrophic consequences. [fn:19]
** Continual learning
- Champion/Challenger model [fn:18]
  [[./img/champion-challenger.png]]
- Stateless retraining vs Stateful training


* Footnotes

[fn:19] Continual learning is largely an infrastructural problem.
[fn:18] [[https://blog.dataiku.com/mlops-champion-challenger-model-evaluation][MLOps: Champion/Challenger Model Evaluation in Dataiku]], [[https://medium.com/decision-automation/what-is-champion-challenger-and-how-does-it-enable-choosing-the-right-decision-f57b8b653149][What is Champion Challenger and How does It Enable Choosing the Right Decision? | by Arash Aghlara | FlexRule Decision Automation | Medium]]

[fn:7] Vowpal Wabbit is a machine learning library that was developed by John Langford and his research team at Yahoo! Research and later Microsoft Research. The name "Vowpal Wabbit" is a playful corruption of the words "vorpal rabbit," which is a reference to the poem "Jabberwocky" by Lewis Carroll. In the poem, the word "vorpal" is a nonsensical adjective, and "rabbit" is a small mammal. In the context of the machine learning library, "Vowpal Wabbit" doesn't have a specific verbal meaning but serves as a whimsical name for the project.
[fn:17] [[https://developers.google.com/machine-learning/guides/rules-of-ml#rule_36_avoid_feedback_loops_with_positional_features][Rules of Machine Learning:  |  Google for Developers]]
[fn:10] [[https://towardsdatascience.com/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf][Biases in Recommender Systems: Top Challenges and Recent Breakthroughs | by Samuel Flender | Towards Data Science]], [[https://towardsdatascience.com/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9][Machine Learning Does Not Only Predict the Future, It Actively Creates It | by Samuel Flender | Towards Data Science]]
[fn:8] [[https://en.wikipedia.org/wiki/Multi-armed_bandit][Multi-armed bandit - Wikipedia]]
[fn:2] [[https://medium.com/p/7895bb1ab423][Why TikTok made its user so obsessive? The AI Algorithm that got you hooked. | by Catherine Wang | Towards Data Science]]
[fn:16]
- https://www.kaggle.com/code/vipulgandhi/a-comprehensive-guide-to-ensemble-learning
- https://huggingface.co/docs/transformers/v4.18.0/en/performance
[fn:15]
- https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions
- [[https://en.wikipedia.org/wiki/Multiple-criteria_decision_analysis][Multiple-criteria decision analysis - Wikipedia]]
[fn:14] [[https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087][Don’t be tricked by the Hashing Trick | by Lucas Bernardi | Booking.com Data Science]]
[fn:13] http://web.stanford.edu/class/cs246/
[fn:12] https://stanford-cs329s.github.io/syllabus.html
[fn:11]
- https://huggingface.co/spaces/merve/uncertainty-calibration
- https://www.kaggle.com/code/mateuscco/how-to-evaluate-model-calibration
- https://www.kaggle.com/code/naiborhujosua/model-calibration
- https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html
- https://github.com/gpleiss/temperature_scaling
[fn:9] [[https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26][How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs | by Quoc N. Le | Medium]]
[fn:6] [[https://magazine.sebastianraschka.com/p/finetuning-large-language-models][Finetuning Large Language Models]], [[https://medium.com/@abonia/llm-series-parameter-efficient-fine-tuning-e9839fae44ac][LLM Series — Parameter Efficient Fine Tuning | by Abonia Sojasingarayar | Jun, 2023 | Medium]], https://github.com/huggingface/peft, [[https://lightning.ai/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/][How To Finetune GPT Like Large Language Models on a Custom Dataset - Lightning AI]], [[https://lightning.ai/pages/community/article/lora-llm/][Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA) - Lightning AI]]
[fn:5]
- [[https://arxiv.org/abs/2304.01933][[2304.01933] LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models]]
- [[https://arxiv.org/abs/2306.10723][[2306.10723] Fine-tuning Large Enterprise Language Models via Ontological Reasoning]]
- [[https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters][Finetuning LLMs Efficiently with Adapters]]
[fn:4] Using 16 bits to represent a float is called half precision. Instead of using floats, you can have a model entirely in integers; each integer takes only 8 bits to represent. This method is also known as - https://huggingface.co/docs/optimum/concept_guides/quantization
- https://arxiv.org/abs/1906.00532
- [[https://www.mathworks.com/company/newsletters/articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html][What Is int8 - Quantization and Why Is It Popular for Deep Neural Networks? - MATLAB & Simulink]]
- https://www.kaggle.com/code/anilkrsah/model-quantization
- https://www.kaggle.com/code/ritvik1909/model-quantization
[fn:3]
- https://github.com/cybertronai/gradient-checkpointing
- [[https://web.stanford.edu/~rezab/classes/cme323/S20/][DAO: Distributed Algorithms and Optimization]]
[fn:1]
Chip Huyen, Designing Machine Learning Systems, p.138
* References
#+CITE_EXPORT: csl /Users/toeinriver/Desktop/ml-system-design/csl/ieee.csl
#+print_bibliography:
